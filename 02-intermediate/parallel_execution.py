"""
02-intermediate: å¹¶è¡Œæ‰§è¡Œå¤„ç†

æœ¬ç¤ºä¾‹å±•ç¤ºLangGraphä¸­å¹¶è¡Œæ‰§è¡Œçš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä»»åŠ¡åˆ†å‰ã€
å¹¶è¡Œå¤„ç†å’Œç»“æœåˆå¹¶ç­‰é«˜çº§ç‰¹æ€§ã€‚

å­¦ä¹ è¦ç‚¹ï¼š
1. å¹¶è¡ŒèŠ‚ç‚¹å®šä¹‰
2. æ•°æ®åˆ†å‰å’Œåˆå¹¶
3. å¼‚æ­¥ä»»åŠ¡å¤„ç†
4. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
"""

from typing import TypedDict, Literal, Dict, List, Any
from langgraph.graph import StateGraph, END
import sys
import os
import time
import asyncio
import concurrent.futures
import random

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils import print_step, print_result, print_error

# 1. çŠ¶æ€å®šä¹‰
class ParallelState(TypedDict):
    """
    å¹¶è¡Œæ‰§è¡Œå·¥ä½œæµçŠ¶æ€
    """
    input_data: Dict[str, Any]
    parallel_results: Dict[str, Any]
    merged_result: Dict[str, Any]
    execution_times: Dict[str, float]
    task_status: Dict[str, str]
    total_time: float

class DataProcessingState(TypedDict):
    """
    æ•°æ®å¤„ç†å¹¶è¡ŒçŠ¶æ€
    """
    raw_data: List[Dict[str, Any]]
    processed_chunks: List[Dict[str, Any]]
    analysis_results: Dict[str, Any]
    final_report: str
    processing_stats: Dict[str, Any]

class AnalysisState(TypedDict):
    """
    åˆ†æå¹¶è¡ŒçŠ¶æ€
    """
    source_data: Dict[str, Any]
    sentiment_result: Dict[str, Any]
    classification_result: Dict[str, Any]
    extraction_result: Dict[str, Any]
    combined_analysis: Dict[str, Any]

# 2. å¹¶è¡Œå¤„ç†å‡½æ•°

def simulate_processing_task(task_name: str, duration_range: tuple = (1, 3)) -> Dict[str, Any]:
    """
    æ¨¡æ‹Ÿå¤„ç†ä»»åŠ¡
    """
    start_time = time.time()
    duration = random.uniform(*duration_range)
    
    print(f"ğŸ”„ å¼€å§‹æ‰§è¡Œ {task_name} (é¢„è®¡ {duration:.1f}s)")
    
    # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    time.sleep(duration)
    
    end_time = time.time()
    execution_time = end_time - start_time
    
    result = {
        "task_name": task_name,
        "status": "completed",
        "execution_time": execution_time,
        "output": f"{task_name} çš„å¤„ç†ç»“æœ",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    print(f"âœ… å®Œæˆæ‰§è¡Œ {task_name} (è€—æ—¶ {execution_time:.1f}s)")
    
    return result

def data_analyzer(data: Dict[str, Any]) -> Dict[str, Any]:
    """æ•°æ®åˆ†æä»»åŠ¡"""
    time.sleep(random.uniform(0.5, 1.5))
    
    numeric_values = [v for v in data.values() if isinstance(v, (int, float))]
    text_values = [v for v in data.values() if isinstance(v, str)]
    
    return {
        "analysis_type": "data_analysis",
        "numeric_count": len(numeric_values),
        "text_count": len(text_values),
        "total_items": len(data),
        "numeric_sum": sum(numeric_values) if numeric_values else 0,
        "numeric_avg": sum(numeric_values) / len(numeric_values) if numeric_values else 0
    }

def sentiment_analyzer(text_data: Dict[str, Any]) -> Dict[str, Any]:
    """æƒ…æ„Ÿåˆ†æä»»åŠ¡"""
    time.sleep(random.uniform(1.0, 2.0))
    
    texts = [v for v in text_data.values() if isinstance(v, str)]
    
    # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æ
    sentiments = []
    for text in texts:
        if len(text) % 3 == 0:
            sentiment = "positive"
        elif len(text) % 3 == 1:
            sentiment = "neutral"
        else:
            sentiment = "negative"
        sentiments.append(sentiment)
    
    return {
        "analysis_type": "sentiment_analysis",
        "analyzed_texts": len(texts),
        "sentiments": sentiments,
        "positive_count": sentiments.count("positive"),
        "negative_count": sentiments.count("negative"),
        "neutral_count": sentiments.count("neutral")
    }

def keyword_extractor(text_data: Dict[str, Any]) -> Dict[str, Any]:
    """å…³é”®è¯æå–ä»»åŠ¡"""
    time.sleep(random.uniform(0.8, 1.8))
    
    # æ¨¡æ‹Ÿå…³é”®è¯æå–
    all_text = " ".join([str(v) for v in text_data.values()])
    words = all_text.split()
    
    # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå–å‰5ä¸ªæœ€é•¿çš„è¯ï¼‰
    keywords = sorted(set(words), key=len, reverse=True)[:5]
    
    return {
        "analysis_type": "keyword_extraction",
        "total_words": len(words),
        "unique_words": len(set(words)),
        "extracted_keywords": keywords,
        "keyword_count": len(keywords)
    }

# 3. å¹¶è¡Œæ‰§è¡ŒèŠ‚ç‚¹

def parallel_data_processing(state: ParallelState) -> ParallelState:
    """
    å¹¶è¡Œæ•°æ®å¤„ç†èŠ‚ç‚¹
    """
    print_step("å¼€å§‹å¹¶è¡Œæ•°æ®å¤„ç†")
    
    input_data = state.get("input_data", {})
    parallel_results = state.get("parallel_results", {})
    execution_times = state.get("execution_times", {})
    
    # å®šä¹‰å¹¶è¡Œä»»åŠ¡
    tasks = {
        "task1": lambda: simulate_processing_task("æ•°æ®æ¸…æ´—", (1, 2)),
        "task2": lambda: simulate_processing_task("æ•°æ®è½¬æ¢", (0.5, 1.5)),
        "task3": lambda: simulate_processing_task("æ•°æ®éªŒè¯", (0.8, 1.2))
    }
    
    # ä½¿ç”¨ThreadPoolExecutoræ‰§è¡Œå¹¶è¡Œä»»åŠ¡
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {
            executor.submit(task_func): task_name 
            for task_name, task_func in tasks.items()
        }
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_task):
            task_name = future_to_task[future]
            try:
                result = future.result()
                parallel_results[task_name] = result
                execution_times[task_name] = result["execution_time"]
            except Exception as exc:
                print(f"ä»»åŠ¡ {task_name} æ‰§è¡Œå¤±è´¥: {exc}")
                parallel_results[task_name] = {"status": "failed", "error": str(exc)}
                execution_times[task_name] = 0.0
    
    print(f"å¹¶è¡Œå¤„ç†å®Œæˆï¼Œå…±æ‰§è¡Œ {len(parallel_results)} ä¸ªä»»åŠ¡")
    
    return {
        "parallel_results": parallel_results,
        "execution_times": execution_times,
        "task_status": {k: v.get("status", "unknown") for k, v in parallel_results.items()}
    }

def parallel_analysis(state: ParallelState) -> ParallelState:
    """
    å¹¶è¡Œåˆ†æèŠ‚ç‚¹
    """
    print_step("å¼€å§‹å¹¶è¡Œåˆ†æ")
    
    input_data = state.get("input_data", {})
    parallel_results = state.get("parallel_results", {})
    execution_times = state.get("execution_times", {})
    
    # å¹¶è¡Œåˆ†æä»»åŠ¡
    analysis_tasks = {
        "data_analysis": lambda: data_analyzer(input_data),
        "sentiment_analysis": lambda: sentiment_analyzer(input_data),
        "keyword_extraction": lambda: keyword_extractor(input_data)
    }
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        future_to_task = {
            executor.submit(task_func): task_name 
            for task_name, task_func in analysis_tasks.items()
        }
        
        for future in concurrent.futures.as_completed(future_to_task):
            task_name = future_to_task[future]
            try:
                result = future.result()
                parallel_results[task_name] = result
                execution_times[task_name] = time.time()  # è®°å½•å®Œæˆæ—¶é—´
            except Exception as exc:
                print(f"åˆ†æä»»åŠ¡ {task_name} å¤±è´¥: {exc}")
                parallel_results[task_name] = {"analysis_type": task_name, "error": str(exc)}
    
    print(f"å¹¶è¡Œåˆ†æå®Œæˆ")
    
    return {
        "parallel_results": parallel_results,
        "execution_times": execution_times
    }

def results_merger(state: ParallelState) -> ParallelState:
    """
    ç»“æœåˆå¹¶èŠ‚ç‚¹
    """
    print_step("åˆå¹¶å¹¶è¡Œç»“æœ")
    
    parallel_results = state.get("parallel_results", {})
    execution_times = state.get("execution_times", {})
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    merged_result = {
        "summary": {
            "total_tasks": len(parallel_results),
            "successful_tasks": len([r for r in parallel_results.values() 
                                   if r.get("status") == "completed"]),
            "failed_tasks": len([r for r in parallel_results.values() 
                                if r.get("status") == "failed"]),
            "total_execution_time": sum(execution_times.values())
        },
        "detailed_results": parallel_results,
        "performance_metrics": {
            "average_task_time": sum(execution_times.values()) / len(execution_times) 
                                 if execution_times else 0,
            "fastest_task": min(execution_times.items(), key=lambda x: x[1]) 
                           if execution_times else ("none", 0),
            "slowest_task": max(execution_times.items(), key=lambda x: x[1]) 
                           if execution_times else ("none", 0)
        }
    }
    
    print_result(f"ç»“æœåˆå¹¶å®Œæˆ")
    print(f"  - æ€»ä»»åŠ¡æ•°: {merged_result['summary']['total_tasks']}")
    print(f"  - æˆåŠŸä»»åŠ¡: {merged_result['summary']['successful_tasks']}")
    print(f"  - å¤±è´¥ä»»åŠ¡: {merged_result['summary']['failed_tasks']}")
    print(f"  - æ€»è€—æ—¶: {merged_result['summary']['total_execution_time']:.2f}s")
    
    return {
        "merged_result": merged_result
    }

# 4. æ•°æ®å¤„ç†å¹¶è¡ŒèŠ‚ç‚¹

def data_splitter(state: DataProcessingState) -> DataProcessingState:
    """
    æ•°æ®åˆ†å‰²èŠ‚ç‚¹
    """
    print_step("åˆ†å‰²æ•°æ®ä»¥è¿›è¡Œå¹¶è¡Œå¤„ç†")
    
    raw_data = state.get("raw_data", [])
    
    # å°†æ•°æ®åˆ†æˆå¤šä¸ªå—
    chunk_size = max(1, len(raw_data) // 3)
    chunks = [raw_data[i:i + chunk_size] for i in range(0, len(raw_data), chunk_size)]
    
    print(f"åŸå§‹æ•°æ®: {len(raw_data)} æ¡è®°å½•")
    print(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")
    
    return {
        "processed_chunks": chunks
    }

def parallel_chunk_processor(state: DataProcessingState) -> DataProcessingState:
    """
    å¹¶è¡Œå—å¤„ç†èŠ‚ç‚¹
    """
    print_step("å¹¶è¡Œå¤„ç†æ•°æ®å—")
    
    chunks = state.get("processed_chunks", [])
    processed_results = []
    
    def process_chunk(chunk: List[Dict[str, Any]], chunk_id: int) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ•°æ®å—"""
        start_time = time.time()
        
        # æ¨¡æ‹Ÿå¤„ç†é€»è¾‘
        processed_items = []
        for item in chunk:
            processed_item = {
                **item,
                "processed": True,
                "chunk_id": chunk_id,
                "processed_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            processed_items.append(processed_item)
        
        time.sleep(random.uniform(0.5, 1.5))  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        end_time = time.time()
        
        return {
            "chunk_id": chunk_id,
            "processed_items": processed_items,
            "item_count": len(processed_items),
            "processing_time": end_time - start_time
        }
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰å—
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [
            executor.submit(process_chunk, chunk, i) 
            for i, chunk in enumerate(chunks)
        ]
        
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            processed_results.append(result)
            print(f"å— {result['chunk_id']} å¤„ç†å®Œæˆ: {result['item_count']} æ¡è®°å½•")
    
    return {
        "processed_chunks": processed_results
    }

def parallel_analyzer(state: DataProcessingState) -> DataProcessingState:
    """
    å¹¶è¡Œåˆ†æèŠ‚ç‚¹
    """
    print_step("å¹¶è¡Œåˆ†æå¤„ç†ç»“æœ")
    
    chunks = state.get("processed_chunks", [])
    analysis_results = {}
    
    # å®šä¹‰åˆ†æä»»åŠ¡
    def analyze_statistics(chunks):
        """ç»Ÿè®¡åˆ†æ"""
        all_items = []
        for chunk in chunks:
            all_items.extend(chunk.get("processed_items", []))
        
        return {
            "total_items": len(all_items),
            "unique_chunks": len(set(item.get("chunk_id") for item in all_items)),
            "processing_times": [chunk.get("processing_time", 0) for chunk in chunks]
        }
    
    def analyze_quality(chunks):
        """è´¨é‡åˆ†æ"""
        total_items = sum(chunk.get("item_count", 0) for chunk in chunks)
        processing_times = [chunk.get("processing_time", 0) for chunk in chunks]
        
        return {
            "total_items_processed": total_items,
            "average_processing_time": sum(processing_times) / len(processing_times) if processing_times else 0,
            "processing_efficiency": total_items / sum(processing_times) if sum(processing_times) > 0 else 0
        }
    
    def analyze_performance(chunks):
        """æ€§èƒ½åˆ†æ"""
        return {
            "total_chunks": len(chunks),
            "processing_times": [chunk.get("processing_time", 0) for chunk in chunks],
            "fastest_chunk": min(chunks, key=lambda x: x.get("processing_time", float('inf'))),
            "slowest_chunk": max(chunks, key=lambda x: x.get("processing_time", 0))
        }
    
    # å¹¶è¡Œæ‰§è¡Œåˆ†æä»»åŠ¡
    analysis_tasks = {
        "statistics": lambda: analyze_statistics(chunks),
        "quality": lambda: analyze_quality(chunks),
        "performance": lambda: analyze_performance(chunks)
    }
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_task = {
            executor.submit(task_func): task_name 
            for task_name, task_func in analysis_tasks.items()
        }
        
        for future in concurrent.futures.as_completed(future_to_task):
            task_name = future_to_task[future]
            try:
                result = future.result()
                analysis_results[task_name] = result
                print(f"åˆ†æä»»åŠ¡ {task_name} å®Œæˆ")
            except Exception as exc:
                print(f"åˆ†æä»»åŠ¡ {task_name} å¤±è´¥: {exc}")
                analysis_results[task_name] = {"error": str(exc)}
    
    return {
        "analysis_results": analysis_results
    }

def report_generator(state: DataProcessingState) -> DataProcessingState:
    """
    æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
    """
    print_step("ç”Ÿæˆå¤„ç†æŠ¥å‘Š")
    
    chunks = state.get("processed_chunks", [])
    analysis_results = state.get("analysis_results", {})
    
    # ç”ŸæˆæŠ¥å‘Š
    report_lines = [
        "=== å¹¶è¡Œæ•°æ®å¤„ç†æŠ¥å‘Š ===",
        f"å¤„ç†çš„æ•°æ®å—æ•°é‡: {len(chunks)}",
        ""
    ]
    
    # ç»Ÿè®¡ä¿¡æ¯
    if "statistics" in analysis_results:
        stats = analysis_results["statistics"]
        report_lines.extend([
            "ã€ç»Ÿè®¡ä¿¡æ¯ã€‘",
            f"- æ€»å¤„ç†é¡¹ç›®æ•°: {stats.get('total_items', 0)}",
            f"- æ¶‰åŠæ•°æ®å—æ•°: {stats.get('unique_chunks', 0)}",
            ""
        ])
    
    # è´¨é‡ä¿¡æ¯
    if "quality" in analysis_results:
        quality = analysis_results["quality"]
        report_lines.extend([
            "ã€è´¨é‡ä¿¡æ¯ã€‘",
            f"- å¤„ç†æ•ˆç‡: {quality.get('processing_efficiency', 0):.2f} items/s",
            f"- å¹³å‡å¤„ç†æ—¶é—´: {quality.get('average_processing_time', 0):.2f}s",
            ""
        ])
    
    # æ€§èƒ½ä¿¡æ¯
    if "performance" in analysis_results:
        perf = analysis_results["performance"]
        report_lines.extend([
            "ã€æ€§èƒ½ä¿¡æ¯ã€‘",
            f"- æ€»æ•°æ®å—: {perf.get('total_chunks', 0)}",
            f"- æœ€å¿«å—å¤„ç†æ—¶é—´: {perf.get('fastest_chunk', {}).get('processing_time', 0):.2f}s",
            f"- æœ€æ…¢å—å¤„ç†æ—¶é—´: {perf.get('slowest_chunk', {}).get('processing_time', 0):.2f}s",
            ""
        ])
    
    report_lines.append("=== æŠ¥å‘Šç»“æŸ ===")
    
    final_report = "\n".join(report_lines)
    
    print_result("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    return {
        "final_report": final_report,
        "processing_stats": {
            "chunks_processed": len(chunks),
            "total_analysis_tasks": len(analysis_results),
            "report_lines": len(report_lines)
        }
    }

# 5. æ„å»ºå¹¶è¡Œæ‰§è¡Œå·¥ä½œæµ

def build_basic_parallel_workflow():
    """æ„å»ºåŸºç¡€å¹¶è¡Œå·¥ä½œæµ"""
    print_step("æ„å»ºåŸºç¡€å¹¶è¡Œå·¥ä½œæµ")
    
    workflow = StateGraph(ParallelState)
    
    workflow.add_node("parallel_process", parallel_data_processing)
    workflow.add_node("merge", results_merger)
    
    workflow.set_entry_point("parallel_process")
    workflow.add_edge("parallel_process", "merge")
    workflow.add_edge("merge", END)
    
    return workflow.compile()

def build_analysis_parallel_workflow():
    """æ„å»ºåˆ†æå¹¶è¡Œå·¥ä½œæµ"""
    print_step("æ„å»ºåˆ†æå¹¶è¡Œå·¥ä½œæµ")
    
    workflow = StateGraph(ParallelState)
    
    workflow.add_node("parallel_analysis", parallel_analysis)
    workflow.add_node("merge", results_merger)
    
    workflow.set_entry_point("parallel_analysis")
    workflow.add_edge("parallel_analysis", "merge")
    workflow.add_edge("merge", END)
    
    return workflow.compile()

def build_data_processing_parallel_workflow():
    """æ„å»ºæ•°æ®å¤„ç†å¹¶è¡Œå·¥ä½œæµ"""
    print_step("æ„å»ºæ•°æ®å¤„ç†å¹¶è¡Œå·¥ä½œæµ")
    
    workflow = StateGraph(DataProcessingState)
    
    workflow.add_node("split", data_splitter)
    workflow.add_node("parallel_process", parallel_chunk_processor)
    workflow.add_node("parallel_analyze", parallel_analyzer)
    workflow.add_node("generate_report", report_generator)
    
    workflow.set_entry_point("split")
    workflow.add_edge("split", "parallel_process")
    workflow.add_edge("parallel_process", "parallel_analyze")
    workflow.add_edge("parallel_analyze", "generate_report")
    workflow.add_edge("generate_report", END)
    
    return workflow.compile()

# 6. æ¼”ç¤ºå‡½æ•°

def demo_basic_parallel():
    """æ¼”ç¤ºåŸºç¡€å¹¶è¡Œå¤„ç†"""
    print_step("åŸºç¡€å¹¶è¡Œå¤„ç†æ¼”ç¤º")
    
    app = build_basic_parallel_workflow()
    
    initial_state = {
        "input_data": {
            "text1": "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬",
            "number1": 42,
            "text2": "å¦ä¸€æ®µæ–‡æœ¬å†…å®¹",
            "number2": 100,
            "text3": "æ›´å¤šçš„æ–‡æœ¬æ•°æ®"
        },
        "parallel_results": {},
        "merged_result": {},
        "execution_times": {},
        "task_status": {},
        "total_time": 0.0
    }
    
    start_time = time.time()
    result = app.invoke(initial_state)
    end_time = time.time()
    
    print_result(f"åŸºç¡€å¹¶è¡Œå¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f}s")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    merged = result.get("merged_result", {})
    if "summary" in merged:
        summary = merged["summary"]
        print(f"ä»»åŠ¡æ‰§è¡Œæƒ…å†µ:")
        print(f"  - æ€»ä»»åŠ¡: {summary.get('total_tasks', 0)}")
        print(f"  - æˆåŠŸ: {summary.get('successful_tasks', 0)}")
        print(f"  - å¤±è´¥: {summary.get('failed_tasks', 0)}")

def demo_analysis_parallel():
    """æ¼”ç¤ºå¹¶è¡Œåˆ†æ"""
    print_step("å¹¶è¡Œåˆ†ææ¼”ç¤º")
    
    app = build_analysis_parallel_workflow()
    
    initial_state = {
        "input_data": {
            "product_review": "è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼Œè´¨é‡å¾ˆå¥½ï¼Œæ¨èè´­ä¹°ï¼",
            "customer_feedback": "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œä½†æ˜¯é…é€é€Ÿåº¦æœ‰å¾…æé«˜",
            "technical_issue": "ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæ€§èƒ½è¡¨ç°ä¼˜ç§€",
            "price_comment": "ä»·æ ¼åˆç†ï¼Œæ€§ä»·æ¯”é«˜"
        },
        "parallel_results": {},
        "merged_result": {},
        "execution_times": {}
    }
    
    start_time = time.time()
    result = app.invoke(initial_state)
    end_time = time.time()
    
    print_result(f"å¹¶è¡Œåˆ†æå®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f}s")
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    parallel_results = result.get("parallel_results", {})
    for task_name, result_data in parallel_results.items():
        print(f"\n{task_name}:")
        for key, value in result_data.items():
            print(f"  {key}: {value}")

def demo_data_processing_parallel():
    """æ¼”ç¤ºæ•°æ®å¤„ç†å¹¶è¡Œ"""
    print_step("æ•°æ®å¤„ç†å¹¶è¡Œæ¼”ç¤º")
    
    app = build_data_processing_parallel_workflow()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    raw_data = [
        {"id": i, "value": random.randint(1, 100), "category": f"cat_{i % 5}"}
        for i in range(15)
    ]
    
    initial_state = {
        "raw_data": raw_data,
        "processed_chunks": [],
        "analysis_results": {},
        "final_report": "",
        "processing_stats": {}
    }
    
    print(f"è¾“å…¥æ•°æ®: {len(raw_data)} æ¡è®°å½•")
    
    start_time = time.time()
    result = app.invoke(initial_state)
    end_time = time.time()
    
    print_result(f"æ•°æ®å¤„ç†å¹¶è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f}s")
    
    # æ˜¾ç¤ºæŠ¥å‘Š
    report = result.get("final_report", "")
    if report:
        print("\n" + "="*50)
        print(report)
        print("="*50)

def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æ¼”ç¤º"""
    print_step("å¹¶è¡Œvsä¸²è¡Œæ€§èƒ½å¯¹æ¯”")
    
    # æ¨¡æ‹Ÿä»»åŠ¡
    def simulate_task(duration):
        time.sleep(duration)
        return f"ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶ {duration}s"
    
    # ä¸²è¡Œæ‰§è¡Œ
    print("æ‰§è¡Œä¸²è¡Œå¤„ç†...")
    serial_start = time.time()
    serial_results = []
    for i in range(3):
        result = simulate_task(random.uniform(0.5, 1.5))
        serial_results.append(result)
    serial_end = time.time()
    serial_time = serial_end - serial_start
    
    print(f"ä¸²è¡Œæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {serial_time:.2f}s")
    
    # å¹¶è¡Œæ‰§è¡Œ
    print("\næ‰§è¡Œå¹¶è¡Œå¤„ç†...")
    parallel_start = time.time()
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(simulate_task, random.uniform(0.5, 1.5)) for _ in range(3)]
        parallel_results = [future.result() for future in concurrent.futures.as_completed(futures)]
    parallel_end = time.time()
    parallel_time = parallel_end - parallel_start
    
    print(f"å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {parallel_time:.2f}s")
    
    # æ€§èƒ½å¯¹æ¯”
    speedup = serial_time / parallel_time if parallel_time > 0 else 0
    print(f"\næ€§èƒ½å¯¹æ¯”:")
    print(f"  ä¸²è¡Œæ—¶é—´: {serial_time:.2f}s")
    print(f"  å¹¶è¡Œæ—¶é—´: {parallel_time:.2f}s")
    print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"  æ•ˆç‡æå‡: {((speedup - 1) * 100):.1f}%")

# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("âš¡ LangGraph å¹¶è¡Œæ‰§è¡Œå­¦ä¹ ç¨‹åº")
    print("=" * 60)
    
    while True:
        print("\nè¯·é€‰æ‹©æ¼”ç¤º:")
        print("1. åŸºç¡€å¹¶è¡Œå¤„ç†")
        print("2. å¹¶è¡Œåˆ†æ")
        print("3. æ•°æ®å¤„ç†å¹¶è¡Œ")
        print("4. æ€§èƒ½å¯¹æ¯”")
        print("0. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-4): ").strip()
        
        if choice == "1":
            demo_basic_parallel()
        elif choice == "2":
            demo_analysis_parallel()
        elif choice == "3":
            demo_data_processing_parallel()
        elif choice == "4":
            performance_comparison()
        elif choice == "0":
            print_step("æ„Ÿè°¢å­¦ä¹ å¹¶è¡Œæ‰§è¡Œï¼")
            break
        else:
            print_error("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
    
    print_result("å¹¶è¡Œæ‰§è¡Œå­¦ä¹ å®Œæˆï¼")