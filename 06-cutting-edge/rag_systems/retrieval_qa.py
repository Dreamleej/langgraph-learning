#!/usr/bin/env python3
"""
RAGæ£€ç´¢å¢å¼ºé—®ç­”ç³»ç»Ÿ
å±•ç¤ºå¦‚ä½•ä½¿ç”¨LangGraphæ„å»ºæ™ºèƒ½æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
"""

import os
import json
import hashlib
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import uuid
import re

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
import numpy as np
from typing_extensions import TypedDict

# å¯¼å…¥é…ç½®
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'utils'))
from config import get_llm


class Document:
    """æ–‡æ¡£ç±»"""
    
    def __init__(self, content: str, metadata: Dict[str, Any] = None):
        self.content = content
        self.metadata = metadata or {}
        self.id = self._generate_id()
        self.embedding = None
        self.chunks = []
    
    def _generate_id(self) -> str:
        """ç”Ÿæˆæ–‡æ¡£ID"""
        content_hash = hashlib.md5(self.content.encode()).hexdigest()
        return f"doc_{content_hash[:12]}"
    
    def chunk_document(self, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """å°†æ–‡æ¡£åˆ†å‰²ä¸ºå—"""
        if not self.content:
            return []
        
        # ç®€å•çš„åˆ†å—ç­–ç•¥
        words = self.content.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk = " ".join(chunk_words)
            chunks.append(chunk)
            
            if len(chunk_words) < chunk_size - overlap:
                break
        
        self.chunks = chunks
        return chunks
    
    def __str__(self):
        return f"Document(id={self.id}, content_length={len(self.content)})"


class VectorStore:
    """ç®€å•çš„å‘é‡å­˜å‚¨"""
    
    def __init__(self):
        self.documents = {}  # doc_id -> Document
        self.embeddings = {}  # chunk_id -> embedding
        self.chunk_mapping = {}  # chunk_id -> doc_id
        self.index = {}  # ç”¨äºå¿«é€Ÿæ£€ç´¢çš„ç´¢å¼•
    
    def add_document(self, document: Document):
        """æ·»åŠ æ–‡æ¡£"""
        self.documents[document.id] = document
        
        # ç”Ÿæˆåˆ†å—
        chunks = document.chunk_document()
        
        # ä¸ºæ¯ä¸ªå—ç”Ÿæˆç®€å•çš„åµŒå…¥ï¼ˆè¿™é‡Œç”¨è¯é¢‘å‘é‡ç®€åŒ–ï¼‰
        for i, chunk in enumerate(chunks):
            chunk_id = f"{document.id}_chunk_{i}"
            embedding = self._simple_embedding(chunk)
            
            self.embeddings[chunk_id] = embedding
            self.chunk_mapping[chunk_id] = document.id
    
    def _simple_embedding(self, text: str) -> np.ndarray:
        """ç®€å•çš„æ–‡æœ¬åµŒå…¥ï¼ˆä½¿ç”¨è¯é¢‘ï¼‰"""
        # ç®€åŒ–çš„TF-IDFé£æ ¼åµŒå…¥
        words = re.findall(r'\w+', text.lower())
        word_count = len(words)
        
        if word_count == 0:
            return np.zeros(100)
        
        # åˆ›å»ºå›ºå®šç»´åº¦çš„å‘é‡
        embedding = np.zeros(100)
        
        # åŸºäºè¯çš„ç®€å•å“ˆå¸Œæ˜ å°„åˆ°å‘é‡ç»´åº¦
        for word in words:
            hash_val = hash(word) % 100
            embedding[hash_val] += 1 / word_count
        
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def similarity_search(self, query: str, k: int = 5) -> List[Tuple[str, float]]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        query_embedding = self._simple_embedding(query)
        similarities = []
        
        for chunk_id, chunk_embedding in self.embeddings.items():
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(query_embedding, chunk_embedding)
            similarities.append((chunk_id, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:k]
    
    def get_document_by_chunk(self, chunk_id: str) -> Optional[Document]:
        """é€šè¿‡å—IDè·å–æ–‡æ¡£"""
        doc_id = self.chunk_mapping.get(chunk_id)
        return self.documents.get(doc_id)
    
    def get_relevant_context(self, query: str, k: int = 3) -> str:
        """è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        search_results = self.similarity_search(query, k)
        
        context_parts = []
        for chunk_id, similarity in search_results:
            doc = self.get_document_by_chunk(chunk_id)
            if doc:
                # æå–å¯¹åº”çš„å—
                chunk_index = int(chunk_id.split("_chunk_")[-1])
                if chunk_index < len(doc.chunks):
                    context_parts.append(f"[æ¥æº: {doc.metadata.get('title', 'æœªçŸ¥')}]\n{doc.chunks[chunk_index]}")
        
        return "\n\n".join(context_parts)


class RAGState(TypedDict):
    """RAGç³»ç»ŸçŠ¶æ€"""
    query: str
    context: str
    relevant_docs: List[Dict[str, Any]]
    response: str
    confidence: float
    sources: List[Dict[str, Any]]
    metadata: Dict[str, Any]


class RAGSystem:
    """RAGæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ"""
    
    def __init__(self):
        self.vector_store = VectorStore()
        self.llm = get_llm()
        self.conversation_history = []
    
    def add_knowledge(self, content: str, title: str = "", source: str = ""):
        """æ·»åŠ çŸ¥è¯†æ–‡æ¡£"""
        metadata = {"title": title, "source": source, "added_at": datetime.now().isoformat()}
        document = Document(content, metadata)
        self.vector_store.add_document(document)
        
        print(f"ğŸ“š æ·»åŠ æ–‡æ¡£: {title or 'æœªçŸ¥æ ‡é¢˜'} ({len(content)} å­—ç¬¦)")
    
    def create_rag_workflow(self) -> StateGraph:
        """åˆ›å»ºRAGå·¥ä½œæµ"""
        
        def query_understanding(state: RAGState) -> RAGState:
            """æŸ¥è¯¢ç†è§£"""
            print_step("ç†è§£æŸ¥è¯¢æ„å›¾")
            query = state.get("query", "")
            
            # ç®€å•çš„æŸ¥è¯¢åˆ†æ
            query_analysis = {
                "original_query": query,
                "query_length": len(query),
                "word_count": len(query.split()),
                "query_type": self._classify_query(query),
                "keywords": self._extract_keywords(query)
            }
            
            return {
                **state,
                "metadata": {**state.get("metadata", {}), "query_analysis": query_analysis}
            }
        
        def knowledge_retrieval(state: RAGState) -> RAGState:
            """çŸ¥è¯†æ£€ç´¢"""
            print_step("æ£€ç´¢ç›¸å…³çŸ¥è¯†")
            query = state.get("query", "")
            
            # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
            search_results = self.vector_store.similarity_search(query, k=5)
            
            # æ„å»ºç›¸å…³æ–‡æ¡£åˆ—è¡¨
            relevant_docs = []
            context_parts = []
            sources = []
            
            for chunk_id, similarity in search_results:
                if similarity > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    doc = self.vector_store.get_document_by_chunk(chunk_id)
                    if doc:
                        chunk_index = int(chunk_id.split("_chunk_")[-1])
                        if chunk_index < len(doc.chunks):
                            chunk_content = doc.chunks[chunk_index]
                            
                            relevant_docs.append({
                                "content": chunk_content,
                                "source": doc.metadata.get("title", "æœªçŸ¥"),
                                "similarity": similarity,
                                "doc_id": doc.id
                            })
                            
                            context_parts.append(chunk_content)
                            sources.append({
                                "title": doc.metadata.get("title", "æœªçŸ¥"),
                                "source": doc.metadata.get("source", ""),
                                "similarity": similarity
                            })
            
            context = "\n\n".join(context_parts)
            
            return {
                **state,
                "context": context,
                "relevant_docs": relevant_docs,
                "sources": sources
            }
        
        def answer_generation(state: RAGState) -> RAGState:
            """ç”Ÿæˆå›ç­”"""
            print_step("ç”Ÿæˆå›ç­”")
            query = state.get("query", "")
            context = state.get("context", "")
            
            # æ„å»ºæç¤ºè¯
            if context:
                prompt = f"""
åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®åŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹
2. æ¡ç†æ¸…æ™°ï¼Œé‡ç‚¹çªå‡º
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜
4. å¼•ç”¨å…·ä½“çš„æ¥æºä¿¡æ¯

å›ç­”ï¼š
"""
            else:
                prompt = f"""
ç”¨æˆ·é—®é¢˜ï¼š{query}

æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚è¯·å°è¯•ï¼š
1. ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯
2. æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®
3. å°è¯•ç›¸å…³é—®é¢˜

å¦‚æœæ‚¨éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œè¯·å…·ä½“è¯´æ˜æ‚¨çš„éœ€æ±‚ã€‚
"""
            
            try:
                response = self.llm.invoke(prompt)
                generated_response = response.content
                
                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = self._calculate_confidence(state, generated_response)
                
            except Exception as e:
                generated_response = f"ç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
                confidence = 0.0
            
            return {
                **state,
                "response": generated_response,
                "confidence": confidence
            }
        
        def response_refinement(state: RAGState) -> RAGState:
            """å›ç­”ä¼˜åŒ–"""
            print_step("ä¼˜åŒ–å›ç­”")
            response = state.get("response", "")
            sources = state.get("sources", [])
            confidence = state.get("confidence", 0.0)
            
            # å¦‚æœæœ‰æ¥æºä¿¡æ¯ï¼Œæ·»åŠ å¼•ç”¨
            if sources and confidence > 0.5:
                source_list = []
                for i, source in enumerate(sources[:3], 1):
                    source_list.append(f"{i}. {source['title']} (ç›¸ä¼¼åº¦: {source['similarity']:.2f})")
                
                refined_response = f"{response}\n\nğŸ“š å‚è€ƒæ¥æº:\n" + "\n".join(source_list)
            else:
                refined_response = response
            
            # æ·»åŠ ç½®ä¿¡åº¦æç¤º
            if confidence < 0.7:
                refined_response += f"\n\nâš ï¸ å›ç­”ç½®ä¿¡åº¦: {confidence:.1%}ï¼Œå»ºè®®ç»“åˆå…¶ä»–ä¿¡æ¯éªŒè¯ã€‚"
            
            return {
                **state,
                "response": refined_response
            }
        
        # æ„å»ºå·¥ä½œæµ
        workflow = StateGraph(RAGState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("query_understanding", query_understanding)
        workflow.add_node("knowledge_retrieval", knowledge_retrieval)
        workflow.add_node("answer_generation", answer_generation)
        workflow.add_node("response_refinement", response_refinement)
        
        # æ·»åŠ è¾¹
        workflow.add_edge(START, "query_understanding")
        workflow.add_edge("query_understanding", "knowledge_retrieval")
        workflow.add_edge("knowledge_retrieval", "answer_generation")
        workflow.add_edge("answer_generation", "response_refinement")
        workflow.add_edge("response_refinement", END)
        
        # ä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹
        memory = MemorySaver()
        return workflow.compile(checkpointer=memory)
    
    def _classify_query(self, query: str) -> str:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["ä»€ä¹ˆæ˜¯", "ä»‹ç»", "å®šä¹‰", "æ¦‚å¿µ"]):
            return "definition"
        elif any(word in query_lower for word in ["å¦‚ä½•", "æ€ä¹ˆ", "æ­¥éª¤", "æ–¹æ³•"]):
            return "how_to"
        elif any(word in query_lower for word in ["ä¸ºä»€ä¹ˆ", "åŸå› ", "åŸç†"]):
            return "why"
        elif any(word in query_lower for word in ["æ¯”è¾ƒ", "åŒºåˆ«", "ä¼˜ç¼ºç‚¹"]):
            return "comparison"
        else:
            return "general"
    
    def _extract_keywords(self, query: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        words = re.findall(r'\w+', query.lower())
        # è¿‡æ»¤åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'å¦‚æœ', 'é‚£ä¹ˆ', 'çš„', 'äº†', 'ç€', 'è¿‡', 'å°†', 'ä¼š', 'èƒ½', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'the', 'is', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        
        keywords = [word for word in words if word not in stop_words and len(word) > 1]
        return keywords[:10]  # è¿”å›å‰10ä¸ªå…³é”®è¯
    
    def _calculate_confidence(self, state: RAGState, response: str) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        base_confidence = 0.5
        
        # åŸºäºæ£€ç´¢ç»“æœè°ƒæ•´
        relevant_docs = state.get("relevant_docs", [])
        if relevant_docs:
            max_similarity = max(doc.get("similarity", 0) for doc in relevant_docs)
            base_confidence += max_similarity * 0.3
        
        # åŸºäºä¸Šä¸‹æ–‡é•¿åº¦è°ƒæ•´
        context = state.get("context", "")
        if len(context) > 100:
            base_confidence += 0.1
        
        # åŸºäºå›ç­”é•¿åº¦è°ƒæ•´
        if len(response) > 50:
            base_confidence += 0.1
        
        return min(base_confidence, 1.0)
    
    def query(self, question: str) -> Dict[str, Any]:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        workflow = self.create_rag_workflow()
        
        initial_state = {
            "query": question,
            "context": "",
            "relevant_docs": [],
            "response": "",
            "confidence": 0.0,
            "sources": [],
            "metadata": {}
        }
        
        # è¿è¡Œå·¥ä½œæµ
        config = {"configurable": {"thread_id": str(uuid.uuid4())}}
        result = workflow.invoke(initial_state, config=config)
        
        # ä¿å­˜åˆ°å¯¹è¯å†å²
        self.conversation_history.append({
            "query": question,
            "response": result.get("response", ""),
            "confidence": result.get("confidence", 0.0),
            "timestamp": datetime.now().isoformat()
        })
        
        return result


def print_step(step: str):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"ğŸ” {step}")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 50)


def demo_rag_system():
    """æ¼”ç¤ºRAGç³»ç»Ÿ"""
    print("ğŸ” RAGæ£€ç´¢å¢å¼ºé—®ç­”ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = RAGSystem()
    
    # æ·»åŠ çŸ¥è¯†åº“
    knowledge_base = [
        {
            "title": "LangGraphä»‹ç»",
            "content": """
LangGraphæ˜¯LangChainç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸€ä¸ªé‡è¦ç»„ä»¶ï¼Œä¸“é—¨ç”¨äºæ„å»ºçŠ¶æ€å›¾å’Œå·¥ä½œæµã€‚
å®ƒæä¾›äº†ä¸€ç§å£°æ˜å¼çš„æ–¹å¼æ¥å®šä¹‰å¤æ‚çš„AIåº”ç”¨æµç¨‹ï¼Œæ”¯æŒæ¡ä»¶è·¯ç”±ã€å¹¶è¡Œæ‰§è¡Œã€
çŠ¶æ€ç®¡ç†å’Œé”™è¯¯å¤„ç†ç­‰é«˜çº§ç‰¹æ€§ã€‚LangGraphç‰¹åˆ«é€‚åˆæ„å»ºéœ€è¦å¤šæ­¥éª¤å¤„ç†çš„AIåº”ç”¨ï¼Œ
å¦‚å¯¹è¯ç³»ç»Ÿã€å†³ç­–æµç¨‹å’Œè‡ªåŠ¨åŒ–å·¥ä½œæµã€‚å®ƒçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå¯è§†åŒ–çš„æµç¨‹å®šä¹‰å’Œå¼ºå¤§çš„çŠ¶æ€ç®¡ç†èƒ½åŠ›ã€‚
""",
            "source": "LangGraphå®˜æ–¹æ–‡æ¡£"
        },
        {
            "title": "LangGraphçš„æ ¸å¿ƒæ¦‚å¿µ",
            "content": """
LangGraphçš„æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬èŠ‚ç‚¹ï¼ˆNodeï¼‰ã€è¾¹ï¼ˆEdgeï¼‰å’ŒçŠ¶æ€ï¼ˆStateï¼‰ã€‚
èŠ‚ç‚¹ä»£è¡¨å¤„ç†å•å…ƒï¼Œå¯ä»¥æ˜¯LLMè°ƒç”¨ã€å·¥å…·ä½¿ç”¨æˆ–æ•°æ®å¤„ç†ï¼›è¾¹å®šä¹‰äº†èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å…³ç³»ï¼›
çŠ¶æ€æ˜¯åœ¨æ•´ä¸ªå·¥ä½œæµä¸­ä¼ é€’çš„æ•°æ®ã€‚LangGraphè¿˜æ”¯æŒæ¡ä»¶è¾¹ï¼ˆConditional Edgeï¼‰ï¼Œ
å¯ä»¥æ ¹æ®è¿è¡Œæ—¶çŠ¶æ€åŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†æ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰æœºåˆ¶ï¼Œ
æ”¯æŒçŠ¶æ€çš„æŒä¹…åŒ–å’Œæ¢å¤ã€‚
""",
            "source": "LangGraphæ•™ç¨‹"
        },
        {
            "title": "RAGç³»ç»ŸåŸç†",
            "content": """
æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼ŒRAGï¼‰æ˜¯ä¸€ç§ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„AIç³»ç»Ÿæ¶æ„ã€‚
RAGç³»ç»Ÿé¦–å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µï¼Œç„¶åå°†è¿™äº›ç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™LLMç”Ÿæˆå›ç­”ã€‚
è¿™ç§æ¶æ„è§£å†³äº†ä¼ ç»ŸLLMçš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šçŸ¥è¯†æ›´æ–°æ»åå’Œå¹»è§‰ç°è±¡ã€‚RAGç³»ç»Ÿé€šè¿‡å®æ—¶æ£€ç´¢æœ€æ–°çš„ç›¸å…³ä¿¡æ¯ï¼Œ
å¤§å¤§æé«˜äº†å›ç­”çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚å…¸å‹çš„RAGç³»ç»ŸåŒ…æ‹¬æ–‡æ¡£é¢„å¤„ç†ã€å‘é‡åŒ–ã€ç›¸ä¼¼åº¦æ£€ç´¢å’Œä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆç­‰æ­¥éª¤ã€‚
""",
            "source": "AIæŠ€æœ¯æ–‡æ¡£"
        }
    ]
    
    # æ·»åŠ çŸ¥è¯†åˆ°ç³»ç»Ÿ
    for knowledge in knowledge_base:
        rag_system.add_knowledge(
            content=knowledge["content"],
            title=knowledge["title"],
            source=knowledge["source"]
        )
    
    print(f"ğŸ“š çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œå…± {len(knowledge_base)} ä¸ªæ–‡æ¡£")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯LangGraphï¼Ÿ",
        "LangGraphæœ‰å“ªäº›æ ¸å¿ƒæ¦‚å¿µï¼Ÿ",
        "RAGç³»ç»Ÿçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ä½¿ç”¨LangGraphæ„å»ºåº”ç”¨ï¼Ÿ",
        "LangGraphå’Œä¼ ç»ŸLLMæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ¯ æŸ¥è¯¢ {i}: {query}")
        print("-" * 40)
        
        result = rag_system.query(query)
        
        response = result.get("response", "")
        confidence = result.get("confidence", 0.0)
        relevant_docs = result.get("relevant_docs", [])
        
        print(f"ğŸ¤– å›ç­”: {response}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {confidence:.1%}")
        
        if relevant_docs:
            print(f"ğŸ“š æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            for j, doc in enumerate(relevant_docs[:2], 1):
                print(f"   {j}. {doc['source']} (ç›¸ä¼¼åº¦: {doc['similarity']:.2f})")
    
    print(f"\nğŸ“ˆ å¯¹è¯å†å²: {len(rag_system.conversation_history)} æ¬¡äº¤äº’")


if __name__ == "__main__":
    try:
        demo_rag_system()
        print("\nâœ… RAGç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()