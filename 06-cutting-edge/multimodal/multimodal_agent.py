#!/usr/bin/env python3
"""
å¤šæ¨¡æ€AIä»£ç†
å±•ç¤ºå¦‚ä½•ä½¿ç”¨LangGraphæ„å»ºå¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„å¤šæ¨¡æ€AIç³»ç»Ÿ
"""

import os
import json
import base64
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
import uuid
import mimetypes
import io
from dataclasses import dataclass

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from typing_extensions import TypedDict

# å¯¼å…¥é…ç½®
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'utils'))
from config import get_llm


@dataclass
class MediaContent:
    """åª’ä½“å†…å®¹ç±»"""
    content: Union[str, bytes]  # æ–‡æœ¬æˆ–äºŒè¿›åˆ¶æ•°æ®
    media_type: str  # text, image, audio, video
    format: str  # å…·ä½“æ ¼å¼ï¼Œå¦‚ jpeg, png, mp3, wav
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
    
    @property
    def is_text(self) -> bool:
        return self.media_type == "text"
    
    @property
    def is_image(self) -> bool:
        return self.media_type == "image"
    
    @property
    def is_audio(self) -> bool:
        return self.media_type == "audio"


class MultimodalState(TypedDict):
    """å¤šæ¨¡æ€ç³»ç»ŸçŠ¶æ€"""
    input_media: List[MediaContent]
    processed_media: Dict[str, Any]
    analysis_results: Dict[str, Any]
    cross_modal_insights: Dict[str, Any]
    final_response: str
    confidence: float
    metadata: Dict[str, Any]


class MediaProcessor:
    """åª’ä½“å¤„ç†å™¨"""
    
    def __init__(self):
        self.llm = get_llm()
        self.processing_history = []
    
    def process_text(self, content: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤„ç†æ–‡æœ¬å†…å®¹"""
        print_step("å¤„ç†æ–‡æœ¬å†…å®¹")
        
        # æ–‡æœ¬åˆ†æ
        analysis = {
            "type": "text",
            "length": len(content),
            "word_count": len(content.split()),
            "language": self._detect_language(content),
            "sentiment": self._analyze_sentiment(content),
            "keywords": self._extract_keywords(content),
            "summary": self._generate_summary(content),
            "entities": self._extract_entities(content),
            "metadata": metadata or {}
        }
        
        return analysis
    
    def process_image(self, image_data: bytes, format: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤„ç†å›¾åƒå†…å®¹"""
        print_step("å¤„ç†å›¾åƒå†…å®¹")
        
        # æ¨¡æ‹Ÿå›¾åƒå¤„ç†ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨è®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼‰
        analysis = {
            "type": "image",
            "format": format,
            "size": len(image_data),
            "description": self._describe_image(image_data),
            "objects": self._detect_objects(image_data),
            "scenes": self._detect_scenes(image_data),
            "colors": self._analyze_colors(image_data),
            "text_content": self._extract_text_from_image(image_data),
            "metadata": metadata or {}
        }
        
        return analysis
    
    def process_audio(self, audio_data: bytes, format: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤„ç†éŸ³é¢‘å†…å®¹"""
        print_step("å¤„ç†éŸ³é¢‘å†…å®¹")
        
        # æ¨¡æ‹ŸéŸ³é¢‘å¤„ç†ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨éŸ³é¢‘å¤„ç†æ¨¡å‹ï¼‰
        analysis = {
            "type": "audio",
            "format": format,
            "size": len(audio_data),
            "duration": self._estimate_duration(audio_data),
            "transcription": self._transcribe_audio(audio_data),
            "speech_emotion": self._analyze_speech_emotion(audio_data),
            "speaker_count": self._detect_speakers(audio_data),
            "language": self._detect_audio_language(audio_data),
            "metadata": metadata or {}
        }
        
        return analysis
    
    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        # ç®€åŒ–çš„è¯­è¨€æ£€æµ‹
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(text) * 0.3:
            return "zh"
        else:
            return "en"
    
    def _analyze_sentiment(self, text: str) -> str:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        positive_words = ["å¥½", "æ£’", "ä¼˜ç§€", "perfect", "good", "great", "excellent"]
        negative_words = ["å·®", "ç³Ÿç³•", "ä¸å¥½", "bad", "terrible", "poor"]
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        words = text.split()
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'the', 'is', 'a', 'an', 'and'}
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        return keywords[:10]
    
    def _generate_summary(self, text: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
        # ç®€å•çš„æ‘˜è¦ç”Ÿæˆ
        sentences = text.split('ã€‚')
        if len(sentences) <= 2:
            return text
        
        # è¿”å›å‰ä¸¤ä¸ªå¥å­ä½œä¸ºæ‘˜è¦
        summary = 'ã€‚'.join(sentences[:2]) + 'ã€‚'
        return summary
    
    def _extract_entities(self, text: str) -> List[Dict[str, str]]:
        """æå–å®ä½“"""
        # ç®€åŒ–çš„å®ä½“æå–
        entities = []
        
        # æŸ¥æ‰¾å¯èƒ½çš„å®ä½“æ¨¡å¼
        import re
        
        # æ—¶é—´æ¨¡å¼
        time_pattern = r'\d{4}å¹´|\d{1,2}æœˆ|\d{1,2}æ—¥'
        times = re.findall(time_pattern, text)
        for time in times:
            entities.append({"type": "TIME", "value": time})
        
        # æ•°å­—æ¨¡å¼
        number_pattern = r'\d+%'
        percentages = re.findall(number_pattern, text)
        for pct in percentages:
            entities.append({"type": "PERCENTAGE", "value": pct})
        
        return entities
    
    def _describe_image(self, image_data: bytes) -> str:
        """æè¿°å›¾åƒå†…å®¹"""
        # æ¨¡æ‹Ÿå›¾åƒæè¿°
        return "è¿™æ˜¯ä¸€å¼ åŒ…å«ä¸°å¯Œè§†è§‰å…ƒç´ çš„å›¾åƒï¼Œå¯èƒ½åŒ…å«äººç‰©ã€ç‰©ä½“æˆ–åœºæ™¯ã€‚"
    
    def _detect_objects(self, image_data: bytes) -> List[str]:
        """æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“"""
        # æ¨¡æ‹Ÿç‰©ä½“æ£€æµ‹
        return ["äººç‰©", "å»ºç­‘", "è‡ªç„¶æ™¯è§‚", "ç‰©å“"]
    
    def _detect_scenes(self, image_data: bytes) -> List[str]:
        """æ£€æµ‹å›¾åƒåœºæ™¯"""
        # æ¨¡æ‹Ÿåœºæ™¯æ£€æµ‹
        return ["å®¤å†…", "å®¤å¤–", "åŸå¸‚", "è‡ªç„¶"]
    
    def _analyze_colors(self, image_data: bytes) -> Dict[str, Any]:
        """åˆ†æå›¾åƒé¢œè‰²"""
        # æ¨¡æ‹Ÿé¢œè‰²åˆ†æ
        return {
            "dominant_colors": ["è“è‰²", "ç»¿è‰²", "çº¢è‰²"],
            "brightness": "ä¸­ç­‰",
            "contrast": "é«˜"
        }
    
    def _extract_text_from_image(self, image_data: bytes) -> str:
        """ä»å›¾åƒä¸­æå–æ–‡å­—"""
        # æ¨¡æ‹ŸOCR
        return "å›¾åƒä¸­åŒ…å«ä¸€äº›æ–‡å­—å†…å®¹"
    
    def _estimate_duration(self, audio_data: bytes) -> float:
        """ä¼°è®¡éŸ³é¢‘æ—¶é•¿"""
        # ç®€åŒ–çš„æ—¶é•¿ä¼°ç®—
        size_mb = len(audio_data) / (1024 * 1024)
        # å‡è®¾å‹ç¼©æ¯”ä¸º10:1ï¼ŒéŸ³é¢‘è´¨é‡128kbps
        estimated_seconds = (size_mb * 8 * 1024 * 1024) / 128000
        return estimated_seconds
    
    def _transcribe_audio(self, audio_data: bytes) -> str:
        """éŸ³é¢‘è½¬å½•"""
        # æ¨¡æ‹Ÿè¯­éŸ³è¯†åˆ«
        return "è¿™æ˜¯ä»éŸ³é¢‘ä¸­è½¬å½•å‡ºçš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…å«äº†è¯­éŸ³çš„ä¸»è¦ä¿¡æ¯ã€‚"
    
    def _analyze_speech_emotion(self, audio_data: bytes) -> str:
        """åˆ†æè¯­éŸ³æƒ…æ„Ÿ"""
        # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æ
        return "neutral"
    
    def _detect_speakers(self, audio_data: bytes) -> int:
        """æ£€æµ‹è¯´è¯äººæ•°é‡"""
        # æ¨¡æ‹Ÿè¯´è¯äººæ£€æµ‹
        return 1
    
    def _detect_audio_language(self, audio_data: bytes) -> str:
        """æ£€æµ‹éŸ³é¢‘è¯­è¨€"""
        # æ¨¡æ‹Ÿè¯­è¨€æ£€æµ‹
        return "zh"


class CrossModalAnalyzer:
    """è·¨æ¨¡æ€åˆ†æå™¨"""
    
    def __init__(self):
        self.llm = get_llm()
    
    def analyze_cross_modal(self, processed_media: Dict[str, Any]) -> Dict[str, Any]:
        """è·¨æ¨¡æ€åˆ†æ"""
        print_step("æ‰§è¡Œè·¨æ¨¡æ€åˆ†æ")
        
        # æ”¶é›†å„ç§åª’ä½“çš„åˆ†æç»“æœ
        media_types = []
        content_summary = []
        
        for media_id, analysis in processed_media.items():
            media_type = analysis.get("type", "unknown")
            media_types.append(media_type)
            
            if media_type == "text":
                summary = analysis.get("summary", "")
                content_summary.append(f"æ–‡æœ¬å†…å®¹: {summary}")
            elif media_type == "image":
                description = analysis.get("description", "")
                objects = analysis.get("objects", [])
                content_summary.append(f"å›¾åƒæè¿°: {description}, åŒ…å«ç‰©ä½“: {', '.join(objects)}")
            elif media_type == "audio":
                transcription = analysis.get("transcription", "")
                content_summary.append(f"éŸ³é¢‘è½¬å½•: {transcription}")
        
        # ç”Ÿæˆè·¨æ¨¡æ€æ´å¯Ÿ
        cross_modal_insights = {
            "media_types": media_types,
            "content_summary": content_summary,
            "consistency_analysis": self._analyze_consistency(processed_media),
            "complementary_info": self._extract_complementary_info(processed_media),
            "overall_theme": self._identify_overall_theme(processed_media),
            "key_insights": self._generate_key_insights(processed_media)
        }
        
        return cross_modal_insights
    
    def _analyze_consistency(self, processed_media: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå¤šæ¨¡æ€å†…å®¹çš„ä¸€è‡´æ€§"""
        consistency_score = 0.85  # æ¨¡æ‹Ÿä¸€è‡´æ€§åˆ†æ•°
        
        return {
            "score": consistency_score,
            "level": "high" if consistency_score > 0.7 else "medium" if consistency_score > 0.4 else "low",
            "conflicts": [],
            "agreements": ["ä¸»é¢˜ä¸€è‡´", "æƒ…æ„Ÿç›¸ç¬¦"]
        }
    
    def _extract_complementary_info(self, processed_media: Dict[str, Any]) -> List[str]:
        """æå–äº’è¡¥ä¿¡æ¯"""
        return [
            "å›¾åƒæä¾›äº†è§†è§‰ä¸Šä¸‹æ–‡",
            "æ–‡æœ¬è¡¥å……äº†è¯¦ç»†ä¿¡æ¯",
            "éŸ³é¢‘ä¼ è¾¾äº†æƒ…æ„Ÿè‰²å½©"
        ]
    
    def _identify_overall_theme(self, processed_media: Dict[str, Any]) -> str:
        """è¯†åˆ«æ•´ä½“ä¸»é¢˜"""
        # ç®€åŒ–çš„ä¸»é¢˜è¯†åˆ«
        return "å¤šæ¨¡æ€å†…å®¹å±•ç¤ºäº†ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå„ä¸ªæ¨¡æ€ç›¸äº’è¡¥å……ï¼Œå½¢æˆäº†å®Œæ•´çš„è¡¨è¾¾ã€‚"
    
    def _generate_key_insights(self, processed_media: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå…³é”®æ´å¯Ÿ"""
        return [
            "å¤šæ¨¡æ€å†…å®¹æä¾›äº†æ›´ä¸°å¯Œçš„ä¿¡æ¯ç»´åº¦",
            "ä¸åŒæ¨¡æ€ä¹‹é—´å…·æœ‰è‰¯å¥½çš„äº’è¡¥æ€§",
            "æ•´ä½“è¡¨è¾¾æ›´åŠ ç«‹ä½“å’Œå®Œæ•´"
        ]


class MultimodalAgent:
    """å¤šæ¨¡æ€AIä»£ç†"""
    
    def __init__(self):
        self.media_processor = MediaProcessor()
        self.cross_modal_analyzer = CrossModalAnalyzer()
        self.conversation_history = []
    
    def create_multimodal_workflow(self) -> StateGraph:
        """åˆ›å»ºå¤šæ¨¡æ€å·¥ä½œæµ"""
        
        def media_preprocessing(state: MultimodalState) -> MultimodalState:
            """åª’ä½“é¢„å¤„ç†"""
            print_step("é¢„å¤„ç†è¾“å…¥åª’ä½“")
            
            input_media = state.get("input_media", [])
            processed_media = {}
            
            for i, media in enumerate(input_media):
                media_id = f"media_{i}"
                
                if media.is_text:
                    result = self.media_processor.process_text(media.content, media.metadata)
                elif media.is_image:
                    result = self.media_processor.process_image(media.content, media.format, media.metadata)
                elif media.is_audio:
                    result = self.media_processor.process_audio(media.content, media.format, media.metadata)
                else:
                    result = {"type": "unknown", "error": "ä¸æ”¯æŒçš„åª’ä½“ç±»å‹"}
                
                processed_media[media_id] = result
            
            return {
                **state,
                "processed_media": processed_media
            }
        
        def content_analysis(state: MultimodalState) -> MultimodalState:
            """å†…å®¹åˆ†æ"""
            print_step("æ·±åº¦åˆ†æå†…å®¹")
            
            processed_media = state.get("processed_media", {})
            analysis_results = {}
            
            for media_id, analysis in processed_media.items():
                # ä¸ºæ¯ç§åª’ä½“ç±»å‹è¿›è¡Œæ·±åº¦åˆ†æ
                media_type = analysis.get("type", "unknown")
                
                if media_type == "text":
                    analysis_results[media_id] = self._deep_analyze_text(analysis)
                elif media_type == "image":
                    analysis_results[media_id] = self._deep_analyze_image(analysis)
                elif media_type == "audio":
                    analysis_results[media_id] = self._deep_analyze_audio(analysis)
            
            return {
                **state,
                "analysis_results": analysis_results
            }
        
        def cross_modal_integration(state: MultimodalState) -> MultimodalState:
            """è·¨æ¨¡æ€æ•´åˆ"""
            print_step("è·¨æ¨¡æ€ä¿¡æ¯æ•´åˆ")
            
            processed_media = state.get("processed_media", {})
            cross_modal_insights = self.cross_modal_analyzer.analyze_cross_modal(processed_media)
            
            return {
                **state,
                "cross_modal_insights": cross_modal_insights
            }
        
        def response_generation(state: MultimodalState) -> MultimodalState:
            """ç”Ÿæˆæœ€ç»ˆå“åº”"""
            print_step("ç”Ÿæˆå¤šæ¨¡æ€å“åº”")
            
            processed_media = state.get("processed_media", {})
            analysis_results = state.get("analysis_results", {})
            cross_modal_insights = state.get("cross_modal_insights", {})
            
            # æ„å»ºç»¼åˆå“åº”
            response_parts = []
            
            # åª’ä½“ç±»å‹æ¦‚è§ˆ
            media_types = cross_modal_insights.get("media_types", [])
            response_parts.append(f"ğŸ“Š å¤„ç†äº† {len(media_types)} ç§åª’ä½“ç±»å‹: {', '.join(media_types)}")
            
            # å„åª’ä½“åˆ†æç»“æœ
            for media_id, analysis in processed_media.items():
                media_type = analysis.get("type", "unknown")
                if media_type == "text":
                    summary = analysis.get("summary", "")
                    response_parts.append(f"ğŸ“ æ–‡æœ¬åˆ†æ: {summary}")
                elif media_type == "image":
                    description = analysis.get("description", "")
                    response_parts.append(f"ğŸ–¼ï¸ å›¾åƒåˆ†æ: {description}")
                elif media_type == "audio":
                    transcription = analysis.get("transcription", "")
                    response_parts.append(f"ğŸµ éŸ³é¢‘è½¬å½•: {transcription}")
            
            # è·¨æ¨¡æ€æ´å¯Ÿ
            overall_theme = cross_modal_insights.get("overall_theme", "")
            response_parts.append(f"ğŸ” æ•´ä½“æ´å¯Ÿ: {overall_theme}")
            
            # å…³é”®æ´å¯Ÿ
            key_insights = cross_modal_insights.get("key_insights", [])
            if key_insights:
                response_parts.append("ğŸ’¡ å…³é”®å‘ç°:")
                for insight in key_insights:
                    response_parts.append(f"   â€¢ {insight}")
            
            final_response = "\n\n".join(response_parts)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_overall_confidence(processed_media, cross_modal_insights)
            
            return {
                **state,
                "final_response": final_response,
                "confidence": confidence
            }
        
        # æ„å»ºå·¥ä½œæµ
        workflow = StateGraph(MultimodalState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("media_preprocessing", media_preprocessing)
        workflow.add_node("content_analysis", content_analysis)
        workflow.add_node("cross_modal_integration", cross_modal_integration)
        workflow.add_node("response_generation", response_generation)
        
        # æ·»åŠ è¾¹
        workflow.add_edge(START, "media_preprocessing")
        workflow.add_edge("media_preprocessing", "content_analysis")
        workflow.add_edge("content_analysis", "cross_modal_integration")
        workflow.add_edge("cross_modal_integration", "response_generation")
        workflow.add_edge("response_generation", END)
        
        # ä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹
        memory = MemorySaver()
        return workflow.compile(checkpointer=memory)
    
    def _deep_analyze_text(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦æ–‡æœ¬åˆ†æ"""
        return {
            **analysis,
            "readability_score": 0.8,
            "complexity_level": "medium",
            "topic_relevance": 0.9
        }
    
    def _deep_analyze_image(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦å›¾åƒåˆ†æ"""
        return {
            **analysis,
            "visual_complexity": "high",
            "aesthetic_score": 0.75,
            "content_quality": 0.85
        }
    
    def _deep_analyze_audio(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦éŸ³é¢‘åˆ†æ"""
        return {
            **analysis,
            "audio_quality": "good",
            "speech_clarity": 0.8,
            "emotional_expression": "neutral"
        }
    
    def _calculate_overall_confidence(self, processed_media: Dict[str, Any], cross_modal_insights: Dict[str, Any]) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        base_confidence = 0.7
        
        # åŸºäºåª’ä½“æ•°é‡
        media_count = len(processed_media)
        if media_count >= 3:
            base_confidence += 0.1
        
        # åŸºäºä¸€è‡´æ€§åˆ†æ
        consistency = cross_modal_insights.get("consistency_analysis", {})
        consistency_score = consistency.get("score", 0.5)
        base_confidence += consistency_score * 0.2
        
        return min(base_confidence, 1.0)
    
    def process_multimodal_input(self, media_inputs: List[MediaContent]) -> Dict[str, Any]:
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        workflow = self.create_multimodal_workflow()
        
        initial_state = {
            "input_media": media_inputs,
            "processed_media": {},
            "analysis_results": {},
            "cross_modal_insights": {},
            "final_response": "",
            "confidence": 0.0,
            "metadata": {"timestamp": datetime.now().isoformat()}
        }
        
        # è¿è¡Œå·¥ä½œæµ
        config = {"configurable": {"thread_id": str(uuid.uuid4())}}
        result = workflow.invoke(initial_state, config=config)
        
        # ä¿å­˜åˆ°å¯¹è¯å†å²
        self.conversation_history.append({
            "input_media_count": len(media_inputs),
            "media_types": [m.media_type for m in media_inputs],
            "response": result.get("final_response", ""),
            "confidence": result.get("confidence", 0.0),
            "timestamp": datetime.now().isoformat()
        })
        
        return result


def print_step(step: str):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"ğŸ¨ {step}")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 50)


def demo_multimodal_agent():
    """æ¼”ç¤ºå¤šæ¨¡æ€ä»£ç†"""
    print("ğŸ¨ å¤šæ¨¡æ€AIä»£ç†æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå¤šæ¨¡æ€ä»£ç†
    agent = MultimodalAgent()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_cases = [
        {
            "name": "çº¯æ–‡æœ¬è¾“å…¥",
            "media": [
                MediaContent(
                    content="LangGraphæ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIå·¥ä½œæµæ¡†æ¶ï¼Œå®ƒè®©å¼€å‘è€…èƒ½å¤Ÿæ„å»ºå¤æ‚çš„æ™ºèƒ½åº”ç”¨ã€‚é€šè¿‡å›¾å½¢åŒ–çš„æ–¹å¼å®šä¹‰èŠ‚ç‚¹å’Œè¾¹ï¼Œå¯ä»¥å®ç°æ¸…æ™°çš„å¯è§†åŒ–å·¥ä½œæµè®¾è®¡ã€‚",
                    media_type="text",
                    format="plain"
                )
            ]
        },
        {
            "name": "å›¾æ–‡æ··åˆè¾“å…¥",
            "media": [
                MediaContent(
                    content="è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†LangGraphçš„å·¥ä½œæµæ¶æ„å›¾ã€‚",
                    media_type="text",
                    format="plain"
                ),
                MediaContent(
                    content=b"fake_image_data_for_demo",  # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
                    media_type="image",
                    format="jpeg",
                    metadata={"description": "LangGraphæ¶æ„å›¾"}
                )
            ]
        },
        {
            "name": "å…¨æ¨¡æ€è¾“å…¥",
            "media": [
                MediaContent(
                    content="è¿™æ˜¯ä¸€ä¸ªåŒ…å«æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘çš„å¤šæ¨¡æ€ç¤ºä¾‹ã€‚",
                    media_type="text",
                    format="plain"
                ),
                MediaContent(
                    content=b"fake_image_data_for_demo",
                    media_type="image",
                    format="png"
                ),
                MediaContent(
                    content=b"fake_audio_data_for_demo",
                    media_type="audio",
                    format="wav"
                )
            ]
        }
    ]
    
    # æ‰§è¡Œæµ‹è¯•
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ¯ æµ‹è¯•æ¡ˆä¾‹ {i}: {test_case['name']}")
        print("=" * 40)
        
        # å¤„ç†å¤šæ¨¡æ€è¾“å…¥
        result = agent.process_multimodal_input(test_case['media'])
        
        # æ˜¾ç¤ºç»“æœ
        final_response = result.get("final_response", "")
        confidence = result.get("confidence", 0.0)
        processed_media = result.get("processed_media", {})
        
        print(f"ğŸ¤– åˆ†æç»“æœ:")
        print(final_response)
        print(f"\nğŸ“Š ç½®ä¿¡åº¦: {confidence:.1%}")
        print(f"ğŸ“ å¤„ç†çš„åª’ä½“: {len(processed_media)} ç§")
        
        # æ˜¾ç¤ºè¯¦ç»†åˆ†æ
        for media_id, analysis in processed_media.items():
            media_type = analysis.get("type", "unknown")
            print(f"   â€¢ {media_type}: {analysis.get('format', 'unknown')}")
    
    print(f"\nğŸ“ˆ å¯¹è¯å†å²: {len(agent.conversation_history)} æ¬¡äº¤äº’")


if __name__ == "__main__":
    try:
        demo_multimodal_agent()
        print("\nâœ… å¤šæ¨¡æ€ä»£ç†æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()