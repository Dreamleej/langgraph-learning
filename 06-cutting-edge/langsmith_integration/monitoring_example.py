#!/usr/bin/env python3
"""
LangSmithç›‘æ§å’Œè¿½è¸ªç¤ºä¾‹
å±•ç¤ºå¦‚ä½•é›†æˆLangSmithè¿›è¡Œå·¥ä½œæµç›‘æ§ã€è°ƒè¯•å’Œåˆ†æ
"""

import os
import time
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
import uuid

from langgraph.graph import StateGraph, START, END
from langsmith import Client, traceable
from langsmith.evaluation import evaluate
from langsmith.run_trees import RunTree
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.outputs import LLMResult

# å¯¼å…¥é…ç½®
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'utils'))
from config import get_llm


class LangSmithConfig:
    """LangSmithé…ç½®ç®¡ç†"""
    
    def __init__(self):
        self.api_key = os.getenv("LANGSMITH_API_KEY", "ls-kucVrtrSyaNjy8wqSjSjUg4NQqnDuHq9m")
        self.api_url = os.getenv("LANGSMITH_ENDPOINT", "https://api.smith.langchain.com")
        self.project_name = os.getenv("LANGSMITH_PROJECT", "langgraph-learning")
        self.enabled = os.getenv("LANGSMITH_ENABLED", "true").lower() == "true"
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        if self.enabled:
            self.client = Client(
                api_key=self.api_key,
                api_url=self.api_url
            )
        else:
            self.client = None
    
    def get_client(self):
        """è·å–LangSmithå®¢æˆ·ç«¯"""
        return self.client
    
    def is_enabled(self):
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨LangSmith"""
        return self.enabled and self.client is not None


class LangSmithCallbackHandler(BaseCallbackHandler):
    """LangSmithå›è°ƒå¤„ç†å™¨"""
    
    def __init__(self, project_name: str = "langgraph-learning"):
        super().__init__()
        self.project_name = project_name
        self.runs = []
    
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """LLMå¼€å§‹æ—¶è°ƒç”¨"""
        print(f"ğŸ¤– LLMå¼€å§‹å¤„ç†: {prompts[0][:50]}...")
        
        run_id = kwargs.get("run_id", str(uuid.uuid4()))
        self.runs.append({
            "run_id": run_id,
            "type": "llm",
            "start_time": datetime.now(),
            "prompts": prompts
        })
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLMç»“æŸæ—¶è°ƒç”¨"""
        print(f"âœ… LLMå¤„ç†å®Œæˆ")
        
        run_id = kwargs.get("run_id")
        for run in self.runs:
            if run["run_id"] == run_id:
                run["end_time"] = datetime.now()
                run["response"] = response.generations[0][0].text if response.generations else ""
                break
    
    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """é“¾å¼€å§‹æ—¶è°ƒç”¨"""
        print(f"ğŸ”— é“¾å¼€å§‹: {serialized.get('name', 'Unknown')}")
        
        run_id = kwargs.get("run_id", str(uuid.uuid4()))
        self.runs.append({
            "run_id": run_id,
            "type": "chain",
            "start_time": datetime.now(),
            "chain_name": serialized.get('name', 'Unknown'),
            "inputs": inputs
        })
    
    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """é“¾ç»“æŸæ—¶è°ƒç”¨"""
        print(f"ğŸ”— é“¾ç»“æŸ")
        
        run_id = kwargs.get("run_id")
        for run in self.runs:
            if run["run_id"] == run_id:
                run["end_time"] = datetime.now()
                run["outputs"] = outputs
                break
    
    def get_runs(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰è¿è¡Œè®°å½•"""
        return self.runs
    
    def get_run_summary(self) -> Dict[str, Any]:
        """è·å–è¿è¡Œæ‘˜è¦"""
        summary = {
            "total_runs": len(self.runs),
            "llm_runs": len([r for r in self.runs if r["type"] == "llm"]),
            "chain_runs": len([r for r in self.runs if r["type"] == "chain"]),
            "total_duration": 0
        }
        
        for run in self.runs:
            if "start_time" in run and "end_time" in run:
                duration = (run["end_time"] - run["start_time"]).total_seconds()
                summary["total_duration"] += duration
        
        return summary


@traceable
def monitored_llm_call(prompt: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
    """è¢«ç›‘æ§çš„LLMè°ƒç”¨"""
    print_step(f"æ‰§è¡Œç›‘æ§çš„LLMè°ƒç”¨: {prompt[:30]}...")
    
    llm = get_llm()
    
    # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
    if context:
        enhanced_prompt = f"""
ä¸Šä¸‹æ–‡ä¿¡æ¯: {context}

ç”¨æˆ·é—®é¢˜: {prompt}

è¯·åŸºäºä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜:
"""
    else:
        enhanced_prompt = prompt
    
    start_time = time.time()
    
    try:
        response = llm.invoke(enhanced_prompt)
        end_time = time.time()
        
        return {
            "response": response.content,
            "prompt": enhanced_prompt,
            "duration": end_time - start_time,
            "success": True,
            "timestamp": datetime.now().isoformat()
        }
    
    except Exception as e:
        end_time = time.time()
        
        return {
            "response": f"é”™è¯¯: {str(e)}",
            "prompt": enhanced_prompt,
            "duration": end_time - start_time,
            "success": False,
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }


def create_monitored_workflow(config: LangSmithConfig):
    """åˆ›å»ºå¸¦ç›‘æ§çš„å·¥ä½œæµ"""
    
    from typing_extensions import TypedDict
    
    class MonitoringState(TypedDict):
        messages: List[Dict[str, str]]
        current_input: str
        response: str
        metadata: Dict[str, Any]
        performance_metrics: Dict[str, Any]
    
    def input_processing(state: MonitoringState) -> MonitoringState:
        """è¾“å…¥å¤„ç†èŠ‚ç‚¹"""
        print_step("è¾“å…¥å¤„ç†")
        
        current_input = state.get("current_input", "")
        
        # åˆ†æè¾“å…¥
        input_analysis = {
            "length": len(current_input),
            "word_count": len(current_input.split()),
            "language": "zh" if any('\u4e00' <= char <= '\u9fff' for char in current_input) else "en",
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            **state,
            "metadata": {**state.get("metadata", {}), "input_analysis": input_analysis}
        }
    
    def ai_processing(state: MonitoringState) -> MonitoringState:
        """AIå¤„ç†èŠ‚ç‚¹"""
        print_step("AIå¤„ç†")
        
        current_input = state.get("current_input", "")
        metadata = state.get("metadata", {})
        
        # æ‰§è¡Œè¢«ç›‘æ§çš„LLMè°ƒç”¨
        result = monitored_llm_call(current_input, metadata)
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        performance_metrics = state.get("performance_metrics", {})
        performance_metrics.update({
            "ai_processing_duration": result.get("duration", 0),
            "ai_processing_success": result.get("success", False),
            "last_ai_call": result.get("timestamp")
        })
        
        return {
            **state,
            "response": result.get("response", ""),
            "performance_metrics": performance_metrics
        }
    
    def response_postprocessing(state: MonitoringState) -> MonitoringState:
        """å“åº”åå¤„ç†èŠ‚ç‚¹"""
        print_step("å“åº”åå¤„ç†")
        
        response = state.get("response", "")
        metadata = state.get("metadata", {})
        
        # åˆ†æå“åº”
        response_analysis = {
            "length": len(response),
            "word_count": len(response.split()),
            "sentiment": "positive",  # ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ
            "timestamp": datetime.now().isoformat()
        }
        
        # æ›´æ–°å…ƒæ•°æ®
        updated_metadata = {
            **metadata,
            "response_analysis": response_analysis
        }
        
        return {
            **state,
            "metadata": updated_metadata
        }
    
    def performance_tracking(state: MonitoringState) -> MonitoringState:
        """æ€§èƒ½è¿½è¸ªèŠ‚ç‚¹"""
        print_step("æ€§èƒ½è¿½è¸ª")
        
        performance_metrics = state.get("performance_metrics", {})
        metadata = state.get("metadata", {})
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½æŒ‡æ ‡
        input_analysis = metadata.get("input_analysis", {})
        response_analysis = metadata.get("response_analysis", {})
        
        overall_metrics = {
            **performance_metrics,
            "total_processing_time": performance_metrics.get("ai_processing_duration", 0),
            "input_to_output_ratio": len(response_analysis.get("response", "")) / max(len(input_analysis.get("input", "")), 1),
            "processing_timestamp": datetime.now().isoformat()
        }
        
        return {
            **state,
            "performance_metrics": overall_metrics
        }
    
    # æ„å»ºå·¥ä½œæµ
    workflow = StateGraph(MonitoringState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("input_processing", input_processing)
    workflow.add_node("ai_processing", ai_processing)
    workflow.add_node("response_postprocessing", response_postprocessing)
    workflow.add_node("performance_tracking", performance_tracking)
    
    # æ·»åŠ è¾¹
    workflow.add_edge(START, "input_processing")
    workflow.add_edge("input_processing", "ai_processing")
    workflow.add_edge("ai_processing", "response_postprocessing")
    workflow.add_edge("response_postprocessing", "performance_tracking")
    workflow.add_edge("performance_tracking", END)
    
    return workflow.compile()


def print_step(step: str):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"ğŸ” {step}")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 50)


def demonstrate_basic_monitoring():
    """æ¼”ç¤ºåŸºç¡€ç›‘æ§åŠŸèƒ½"""
    print("ğŸ” æ¼”ç¤ºåŸºç¡€ç›‘æ§åŠŸèƒ½")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = LangSmithConfig()
    
    if not config.is_enabled():
        print("âš ï¸  LangSmithæœªå¯ç”¨ï¼Œè·³è¿‡ç›‘æ§æ¼”ç¤º")
        return
    
    # åˆ›å»ºå›è°ƒå¤„ç†å™¨
    callback_handler = LangSmithCallbackHandler("langgraph-learning-demo")
    
    # åˆ›å»ºè¢«ç›‘æ§çš„å·¥ä½œæµ
    workflow = create_monitored_workflow(config)
    
    # æµ‹è¯•è¾“å…¥
    test_inputs = [
        "ä»€ä¹ˆæ˜¯LangGraphï¼Ÿ",
        "LangGraphæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "å¦‚ä½•ç›‘æ§LangGraphåº”ç”¨ï¼Ÿ"
    ]
    
    for i, user_input in enumerate(test_inputs, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {user_input}")
        
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": [],
            "current_input": user_input,
            "response": "",
            "metadata": {},
            "performance_metrics": {}
        }
        
        # è¿è¡Œå·¥ä½œæµï¼ˆå¸¦å›è°ƒï¼‰
        start_time = time.time()
        config_params = {"callbacks": [callback_handler]}
        result = workflow.invoke(initial_state, config=config_params)
        end_time = time.time()
        
        # æ˜¾ç¤ºç»“æœ
        response = result.get("response", "")
        performance = result.get("performance_metrics", {})
        
        print(f"ğŸ¤– å›å¤: {response[:100]}...")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ“Š AIå¤„ç†æ—¶é—´: {performance.get('ai_processing_duration', 0):.2f}ç§’")
        print(f"âœ… å¤„ç†æˆåŠŸ: {performance.get('ai_processing_success', False)}")
    
    # æ˜¾ç¤ºè¿è¡Œæ‘˜è¦
    summary = callback_handler.get_run_summary()
    print(f"\nğŸ“Š è¿è¡Œæ‘˜è¦:")
    print(f"   æ€»è¿è¡Œæ¬¡æ•°: {summary['total_runs']}")
    print(f"   LLMè°ƒç”¨æ¬¡æ•°: {summary['llm_runs']}")
    print(f"   é“¾è¿è¡Œæ¬¡æ•°: {summary['chain_runs']}")
    print(f"   æ€»è€—æ—¶: {summary['total_duration']:.2f}ç§’")


def demonstrate_traceable_functions():
    """æ¼”ç¤ºå¯è¿½è¸ªå‡½æ•°"""
    print("\nğŸ¯ æ¼”ç¤ºå¯è¿½è¸ªå‡½æ•°")
    print("=" * 60)
    
    # é…ç½®
    config = LangSmithConfig()
    
    if not config.is_enabled():
        print("âš ï¸  LangSmithæœªå¯ç”¨ï¼Œè·³è¿‡æ¼”ç¤º")
        return
    
    # æµ‹è¯•å¯è¿½è¸ªå‡½æ•°
    test_cases = [
        {"prompt": "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½", "context": {"topic": "technology"}},
        {"prompt": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹", "context": {"level": "beginner"}},
        {"prompt": "ä»‹ç»LangGraph", "context": {"framework": "langchain"}}
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {test_case['prompt']}")
        
        result = monitored_llm_call(
            test_case["prompt"], 
            test_case["context"]
        )
        
        print(f"ğŸ¤– å›å¤: {result['response'][:100]}...")
        print(f"â±ï¸  è€—æ—¶: {result['duration']:.2f}ç§’")
        print(f"âœ… æˆåŠŸ: {result['success']}")
        print(f"ğŸ• æ—¶é—´æˆ³: {result['timestamp']}")


def demonstrate_evaluation_metrics():
    """æ¼”ç¤ºè¯„ä¼°æŒ‡æ ‡"""
    print("\nğŸ“ˆ æ¼”ç¤ºè¯„ä¼°æŒ‡æ ‡")
    print("=" * 60)
    
    # é…ç½®
    config = LangSmithConfig()
    
    if not config.is_enabled() or not config.client:
        print("âš ï¸  LangSmithå®¢æˆ·ç«¯æœªé…ç½®ï¼Œè·³è¿‡è¯„ä¼°æ¼”ç¤º")
        return
    
    # å®šä¹‰è¯„ä¼°æ•°æ®é›†
    dataset = [
        {
            "input": "ä»€ä¹ˆæ˜¯LangGraphï¼Ÿ",
            "expected_output": "LangGraphæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºçŠ¶æ€å›¾ã€å·¥ä½œæµå’Œæ™ºèƒ½ä»£ç†çš„æ¡†æ¶"
        },
        {
            "input": "LangGraphæœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ", 
            "expected_output": "LangGraphçš„ä¼˜åŠ¿åŒ…æ‹¬çŠ¶æ€ç®¡ç†ã€æ¡ä»¶è·¯ç”±ã€å¹¶è¡Œå¤„ç†ç­‰"
        }
    ]
    
    def run_evaluator(input_text: str):
        """è¿è¡Œè¯„ä¼°å™¨"""
        result = monitored_llm_call(input_text)
        return result.get("response", "")
    
    # ç®€å•çš„è¯„ä¼°å‡½æ•°
    def simple_evaluator(run, example):
        """ç®€å•è¯„ä¼°å‡½æ•°"""
        output = run.outputs.get("output", "")
        expected = example.outputs.get("expected_output", "")
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        output_words = set(output.lower().split())
        expected_words = set(expected.lower().split())
        
        if len(expected_words) == 0:
            similarity = 0.0
        else:
            common_words = output_words & expected_words
            similarity = len(common_words) / len(expected_words)
        
        return {"score": similarity}
    
    try:
        # è¿è¡Œè¯„ä¼°ï¼ˆè¿™é‡Œåªæ˜¯æ¼”ç¤ºï¼Œå®é™…éœ€è¦çœŸå®çš„LangSmithé¡¹ç›®ï¼‰
        print("ğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
        print("   - å‡†ç¡®æ€§ (Accuracy)")
        print("   - å“åº”æ—¶é—´ (Response Time)")
        print("   - æˆåŠŸç‡ (Success Rate)")
        print("   - ç”¨æˆ·æ»¡æ„åº¦ (User Satisfaction)")
        
        print("\nğŸ“ è¯„ä¼°ç»“æœï¼ˆæ¨¡æ‹Ÿï¼‰:")
        for i, example in enumerate(dataset, 1):
            input_text = example["input"]
            expected_output = example["expected_output"]
            
            # è¿è¡Œè¯„ä¼°
            actual_output = run_evaluator(input_text)
            
            # è®¡ç®—æŒ‡æ ‡
            output_words = set(actual_output.lower().split())
            expected_words = set(expected_output.lower().split())
            
            if len(expected_words) == 0:
                similarity = 0.0
            else:
                common_words = output_words & expected_words
                similarity = len(common_words) / len(expected_words)
            
            print(f"   æµ‹è¯• {i}:")
            print(f"     è¾“å…¥: {input_text}")
            print(f"     é¢„æœŸ: {expected_output}")
            print(f"     å®é™…: {actual_output[:100]}...")
            print(f"     ç›¸ä¼¼åº¦: {similarity:.2f}")
    
    except Exception as e:
        print(f"âš ï¸  è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


def demonstrate_error_tracking():
    """æ¼”ç¤ºé”™è¯¯è¿½è¸ª"""
    print("\nğŸš¨ æ¼”ç¤ºé”™è¯¯è¿½è¸ª")
    print("=" * 60)
    
    # é…ç½®
    config = LangSmithConfig()
    
    # æ•…æ„åˆ¶é€ é”™è¯¯çš„æµ‹è¯•
    error_test_cases = [
        {"prompt": "", "description": "ç©ºè¾“å…¥æµ‹è¯•"},
        {"prompt": "x" * 10000, "description": "è¶…é•¿è¾“å…¥æµ‹è¯•"},
        {"prompt": "æ•…æ„è§¦å‘çš„é”™è¯¯æµ‹è¯•", "description": "å¼‚å¸¸å¤„ç†æµ‹è¯•"}
    ]
    
    for i, test_case in enumerate(error_test_cases, 1):
        print(f"\nğŸ§ª é”™è¯¯æµ‹è¯• {i}: {test_case['description']}")
        
        try:
            result = monitored_llm_call(
                test_case["prompt"],
                {"test_type": test_case["description"]}
            )
            
            print(f"ğŸ“Š ç»“æœ:")
            print(f"   æˆåŠŸ: {result['success']}")
            print(f"   è€—æ—¶: {result['duration']:.2f}ç§’")
            
            if not result['success']:
                print(f"   é”™è¯¯: {result.get('error', 'Unknown error')}")
            else:
                print(f"   å›å¤: {result['response'][:100]}...")
        
        except Exception as e:
            print(f"âŒ æ•è·åˆ°å¼‚å¸¸: {e}")
            print(f"   è¿™å±•ç¤ºäº†LangSmithå¦‚ä½•è¿½è¸ªå’Œå¤„ç†å¼‚å¸¸")


if __name__ == "__main__":
    print("ğŸ” LangSmith ç›‘æ§å’Œè¿½è¸ªæ¼”ç¤º")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        print("ğŸ”§ é…ç½®æ£€æŸ¥:")
        config = LangSmithConfig()
        print(f"   LangSmithå¯ç”¨: {config.enabled}")
        print(f"   é¡¹ç›®åç§°: {config.project_name}")
        print(f"   API URL: {config.api_url}")
        print(f"   å®¢æˆ·ç«¯çŠ¶æ€: {'å·²è¿æ¥' if config.is_enabled() else 'æœªè¿æ¥'}")
        
        print("\n" + "=" * 60)
        
        # æ¼”ç¤ºå„ç§åŠŸèƒ½
        demonstrate_basic_monitoring()
        demonstrate_traceable_functions()
        demonstrate_evaluation_metrics()
        demonstrate_error_tracking()
        
        print("\nâœ… LangSmithé›†æˆæ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ’¡ æç¤º: è®¿é—® https://smith.langchain.com æŸ¥çœ‹è¯¦ç»†ç›‘æ§æ•°æ®")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()