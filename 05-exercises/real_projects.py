"""
05-exercises: çœŸå®é¡¹ç›®å®è·µ

è¿™ä¸ªæ–‡ä»¶åŒ…å«å®Œæ•´çš„çœŸå®é¡¹ç›®çº§åˆ«çš„LangGraphåº”ç”¨ç»ƒä¹ ï¼Œ
æ¨¡æ‹Ÿä¼ä¸šçº§åº”ç”¨åœºæ™¯ï¼Œè¦æ±‚æ‚¨ç»¼åˆè¿ç”¨æ‰€å­¦çš„æ‰€æœ‰çŸ¥è¯†ã€‚

é¡¹ç›®åŒ…æ‹¬ï¼š
- æ™ºèƒ½å®¢æœç³»ç»Ÿ
- æ•°æ®åˆ†æå¹³å°
- ä¸šåŠ¡æµç¨‹è‡ªåŠ¨åŒ–
- å¤šæ¨¡æ€AIåº”ç”¨
- å¾®æœåŠ¡ç¼–æ’å¹³å°
"""

from typing import TypedDict, List, Dict, Any, Literal, Optional
from langgraph.graph import StateGraph, END
import sys
import os
import time
import json
import asyncio
import sqlite3
from datetime import datetime, timedelta
from enum import Enum
import random

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils import print_step, print_result, print_error, Config


# ================================
 é¡¹ç›® 1: æ™ºèƒ½å®¢æœå¹³å°
# ================================

def project_1_customer_service_platform():
    """
    é¡¹ç›® 1: å…¨æ¸ é“æ™ºèƒ½å®¢æœå¹³å°
    
    åŠŸèƒ½è¦æ±‚:
    1. å¤šæ¸ é“æ¥å…¥ï¼ˆç½‘ç«™ã€APPã€å¾®ä¿¡ã€ç”µè¯ï¼‰
    2. æ™ºèƒ½è·¯ç”±å’Œåˆ†é…
    3. æœºå™¨äººè‡ªåŠ¨å›å¤
    4. äººå·¥è½¬æ¥å’ŒååŒ
    5. çŸ¥è¯†åº“é›†æˆ
    6. æœåŠ¡è´¨é‡ç›‘æ§
    7. å®¢æˆ·æ»¡æ„åº¦ç®¡ç†
    8. å·¥å•ç³»ç»Ÿé›†æˆ
    
    æŠ€æœ¯æŒ‘æˆ˜:
    - é«˜å¹¶å‘å¤„ç†
    - å®æ—¶æ€§è¦æ±‚
    - å¤šç³»ç»Ÿé›†æˆ
    - ç”¨æˆ·ä½“éªŒä¼˜åŒ–
    """
    
    class CustomerServiceState(TypedDict):
        session_id: str
        customer_id: str
        channel: str
        inquiry_type: str
        priority: str
        customer_profile: Dict[str, Any]
        conversation_history: List[Dict[str, Any]]
        knowledge_search_results: List[Dict[str, Any]]
        agent_assignment: Dict[str, Any]
        auto_resolution: Dict[str, Any]
        escalation_info: Dict[str, Any]
        service_metrics: Dict[str, Any]
        satisfaction_score: float
    
    class ChannelType(Enum):
        WEB = "web"
        APP = "app"
        WECHAT = "wechat"
        PHONE = "phone"
        EMAIL = "email"
    
    class InquiryType(Enum):
        GENERAL = "general"
        TECHNICAL = "technical"
        BILLING = "billing"
        COMPLAINT = "complaint"
        CONSULTATION = "consultation"
    
    def initialize_service_session(state: CustomerServiceState) -> CustomerServiceState:
        """åˆå§‹åŒ–å®¢æœä¼šè¯"""
        session_id = state.get("session_id", "")
        customer_id = state.get("customer_id", "")
        channel = state.get("channel", "")
        
        # å®¢æˆ·ç”»åƒæ„å»º
        customer_profile = build_customer_profile(customer_id, channel)
        
        # ä¼šè¯å†å²åŠ è½½
        conversation_history = load_customer_history(customer_id)
        
        return {
            "customer_profile": customer_profile,
            "conversation_history": conversation_history
        }
    
    def build_customer_profile(customer_id: str, channel: str) -> Dict[str, Any]:
        """æ„å»ºå®¢æˆ·ç”»åƒ"""
        profile = {
            "customer_id": customer_id,
            "channel": channel,
            "vip_level": random.choice(["normal", "silver", "gold", "platinum"]),
            "registration_date": f"2023-{random.randint(1,12):02d}-{random.randint(1,28):02d}",
            "total_orders": random.randint(0, 100),
            "total_spent": random.uniform(0, 10000),
            "preferred_language": random.choice(["ä¸­æ–‡", "English"]),
            "timezone": random.choice(["UTC+8", "UTC+0", "UTC-5"]),
            "contact_preferences": {},
            "service_history": {
                "total_inquiries": random.randint(0, 50),
                "resolved_rate": random.uniform(0.7, 0.95),
                "average_rating": random.uniform(3.5, 5.0),
                "last_contact": time.time() - random.uniform(0, 30*24*3600)
            }
        }
        
        return profile
    
    def load_customer_history(customer_id: str) -> List[Dict[str, Any]]:
        """åŠ è½½å®¢æˆ·å†å²è®°å½•"""
        history = []
        
        for i in range(random.randint(0, 10)):
            session = {
                "session_id": f"hist_{customer_id}_{i}",
                "timestamp": time.time() - random.uniform(0, 365*24*3600),
                "channel": random.choice([c.value for c in ChannelType]),
                "inquiry_type": random.choice([t.value for t in InquiryType]),
                "resolved": random.random() > 0.2,
                "rating": random.uniform(3.0, 5.0),
                "agent_id": f"agent_{random.randint(1, 20)}",
                "duration": random.randint(5, 60)
            }
            history.append(session)
        
        return sorted(history, key=lambda x: x["timestamp"])
    
    def intelligent_routing(state: CustomerServiceState) -> CustomerServiceState:
        """æ™ºèƒ½è·¯ç”±åˆ†é…"""
        customer_profile = state.get("customer_profile", {})
        inquiry_type = state.get("inquiry_type", "")
        priority = state.get("priority", "normal")
        channel = state.get("channel", "")
        
        # è·¯ç”±ç­–ç•¥
        routing_strategy = determine_routing_strategy(customer_profile, inquiry_type, priority)
        
        # é€‰æ‹©åå¸­
        agent_assignment = assign_agent(routing_strategy, inquiry_type, channel)
        
        return {
            "agent_assignment": agent_assignment
        }
    
    def determine_routing_strategy(profile: Dict[str, Any], inquiry_type: str, priority: str) -> str:
        """ç¡®å®šè·¯ç”±ç­–ç•¥"""
        vip_level = profile.get("vip_level", "normal")
        service_history = profile.get("service_history", {})
        resolved_rate = service_history.get("resolved_rate", 0.8)
        
        if vip_level in ["gold", "platinum"] or priority == "urgent":
            return "premium_agent"
        elif resolved_rate < 0.8:
            return "senior_agent"
        elif inquiry_type == "technical":
            return "technical_agent"
        elif inquiry_type == "billing":
            return "billing_agent"
        else:
            return "general_agent"
    
    def assign_agent(strategy: str, inquiry_type: str, channel: str) -> Dict[str, Any]:
        """åˆ†é…åå¸­"""
        agent_pools = {
            "premium_agent": ["agent_gold_1", "agent_gold_2", "agent_platinum_1"],
            "senior_agent": ["agent_senior_1", "agent_senior_2", "agent_senior_3"],
            "technical_agent": ["agent_tech_1", "agent_tech_2"],
            "billing_agent": ["agent_billing_1", "agent_billing_2"],
            "general_agent": ["agent_gen_1", "agent_gen_2", "agent_gen_3"]
        }
        
        available_agents = agent_pools.get(strategy, agent_pools["general_agent"])
        selected_agent = random.choice(available_agents)
        
        return {
            "agent_id": selected_agent,
            "strategy": strategy,
            "assigned_at": time.time(),
            "estimated_wait_time": random.randint(10, 120),
            "channel_compatibility": check_channel_compatibility(selected_agent, channel)
        }
    
    def check_channel_compatibility(agent_id: str, channel: str) -> bool:
        """æ£€æŸ¥åå¸­æ¸ é“å…¼å®¹æ€§"""
        # ç®€åŒ–ï¼šæ‰€æœ‰åå¸­éƒ½æ”¯æŒæ‰€æœ‰æ¸ é“
        return True
    
    def knowledge_base_search(state: CustomerServiceState) -> CustomerServiceState:
        """çŸ¥è¯†åº“æœç´¢"""
        inquiry_type = state.get("inquiry_type", "")
        conversation_history = state.get("conversation_history", [])
        customer_profile = state.get("customer_profile", {})
        
        # ç”Ÿæˆæœç´¢å…³é”®è¯
        search_keywords = generate_search_keywords(inquiry_type, conversation_history)
        
        # æ‰§è¡ŒçŸ¥è¯†åº“æœç´¢
        knowledge_results = search_knowledge_base(search_keywords, customer_profile)
        
        return {
            "knowledge_search_results": knowledge_results
        }
    
    def generate_search_keywords(inquiry_type: str, history: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®è¯"""
        keywords = [inquiry_type]
        
        # ä»å†å²è®°å½•ä¸­æå–å…³é”®è¯
        if history:
            recent_inquiries = [h for h in history[-3:]]  # æœ€è¿‘3æ¬¡è®°å½•
            for inquiry in recent_inquiries:
                if inquiry.get("inquiry_type"):
                    keywords.append(inquiry["inquiry_type"])
        
        # æ·»åŠ å¸¸è§é—®é¢˜å…³é”®è¯
        common_keywords = ["æ•…éšœ", "é€€æ¬¾", "è´¦å•", "æŠ€æœ¯æ”¯æŒ", "äº§å“å’¨è¯¢"]
        keywords.extend(common_keywords[:2])
        
        return list(set(keywords))
    
    def search_knowledge_base(keywords: List[str], profile: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æœç´¢çŸ¥è¯†åº“"""
        results = []
        
        # æ¨¡æ‹ŸçŸ¥è¯†åº“æ¡ç›®
        kb_entries = [
            {
                "id": "kb_001",
                "title": "å¸¸è§æ•…éšœæ’é™¤æŒ‡å—",
                "category": "technical",
                "keywords": ["æ•…éšœ", "æŠ€æœ¯", "troubleshoot"],
                "content": "è¯¦ç»†çš„æŠ€æœ¯æ•…éšœæ’æŸ¥æ­¥éª¤...",
                "relevance_score": 0.95,
                "success_rate": 0.88
            },
            {
                "id": "kb_002", 
                "title": "é€€æ¬¾æ”¿ç­–è¯´æ˜",
                "category": "billing",
                "keywords": ["é€€æ¬¾", "billing", "refund"],
                "content": "é€€æ¬¾æµç¨‹å’Œæ”¿ç­–è¯¦æƒ…...",
                "relevance_score": 0.87,
                "success_rate": 0.92
            },
            {
                "id": "kb_003",
                "title": "äº§å“åŠŸèƒ½ä»‹ç»",
                "category": "general",
                "keywords": ["äº§å“", "åŠŸèƒ½", "features"],
                "content": "å®Œæ•´çš„äº§å“åŠŸèƒ½ä»‹ç»...",
                "relevance_score": 0.78,
                "success_rate": 0.85
            }
        ]
        
        # è®¡ç®—ç›¸å…³æ€§
        for entry in kb_entries:
            relevance = calculate_keyword_relevance(keywords, entry["keywords"])
            entry["relevance_score"] = relevance
            
            # æ ¹æ®å®¢æˆ·ç”»åƒè°ƒæ•´
            if profile.get("vip_level") == "platinum":
                entry["relevance_score"] *= 1.1
        
        # æ’åºå¹¶è¿”å›æœ€ç›¸å…³çš„ç»“æœ
        sorted_entries = sorted(kb_entries, key=lambda x: x["relevance_score"], reverse=True)
        return sorted_entries[:3]
    
    def calculate_keyword_relevance(search_keywords: List[str], entry_keywords: List[str]) -> float:
        """è®¡ç®—å…³é”®è¯ç›¸å…³æ€§"""
        if not search_keywords:
            return 0.0
        
        matches = len(set(search_keywords) & set(entry_keywords))
        return matches / len(search_keywords)
    
    def auto_resolution(state: CustomerServiceState) -> CustomerServiceState:
        """è‡ªåŠ¨è§£å†³å°è¯•"""
        knowledge_results = state.get("knowledge_search_results", [])
        inquiry_type = state.get("inquiry_type", "")
        customer_profile = state.get("customer_profile", {})
        
        # è¯„ä¼°è‡ªåŠ¨è§£å†³çš„å¯èƒ½æ€§
        auto_resolution_capability = assess_auto_resolution(knowledge_results, inquiry_type, customer_profile)
        
        resolution_result = {}
        if auto_resolution_capability["can_auto_resolve"]:
            # å°è¯•è‡ªåŠ¨è§£å†³
            resolution_result = attempt_auto_resolution(auto_resolution_capability, knowledge_results)
        else:
            resolution_result = {
                "auto_resolved": False,
                "reason": "insufficient_knowledge_confidence",
                "recommended_action": "human_intervention"
            }
        
        return {
            "auto_resolution": resolution_result
        }
    
    def assess_auto_resolution(knowledge_results: List[Dict[str, Any]], 
                              inquiry_type: str, profile: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°è‡ªåŠ¨è§£å†³èƒ½åŠ›"""
        if not knowledge_results:
            return {"can_auto_resolve": False, "confidence": 0.0}
        
        best_match = knowledge_results[0]
        confidence = best_match.get("relevance_score", 0.0)
        success_rate = best_match.get("success_rate", 0.0)
        
        # è€ƒè™‘å®¢æˆ·å› ç´ 
        vip_level = profile.get("vip_level", "normal")
        history = profile.get("service_history", {})
        resolved_rate = history.get("resolved_rate", 0.8)
        
        # è°ƒæ•´ç½®ä¿¡åº¦
        if inquiry_type in ["technical", "complaint"]:
            confidence *= 0.7  # å¤æ‚é—®é¢˜é™ä½è‡ªåŠ¨è§£å†³ç½®ä¿¡åº¦
        elif vip_level == "platinum":
            confidence *= 0.8  # VIPå®¢æˆ·è°¨æ…è‡ªåŠ¨è§£å†³
        elif resolved_rate < 0.6:
            confidence *= 0.6  # è§£å†³ç‡ä½çš„å®¢æˆ·è°¨æ…å¤„ç†
        
        return {
            "can_auto_resolve": confidence > 0.75,
            "confidence": confidence,
            "best_kb_article": best_match,
            "factors_considered": ["kb_relevance", "inquiry_type", "vip_level", "history"]
        }
    
    def attempt_auto_resolution(assessment: Dict[str, Any], 
                               knowledge_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å°è¯•è‡ªåŠ¨è§£å†³"""
        best_article = assessment.get("best_kb_article", {})
        confidence = assessment.get("confidence", 0.0)
        
        # æ¨¡æ‹Ÿè‡ªåŠ¨è§£å†³è¿‡ç¨‹
        time.sleep(random.uniform(1, 3))
        
        # æ ¹æ®ç½®ä¿¡åº¦å†³å®šç»“æœ
        if confidence > 0.85:
            success = True
            reason = "high_confidence_match"
        elif confidence > 0.75:
            success = random.random() > 0.2  # 80% æˆåŠŸç‡
            reason = "moderate_confidence_match"
        else:
            success = False
            reason = "low_confidence_match"
        
        return {
            "auto_resolved": success,
            "reason": reason,
            "used_kb_article": best_article.get("id", ""),
            "resolution_time": random.uniform(30, 180),
            "confidence": confidence
        }
    
    def escalation_management(state: CustomerServiceState) -> CustomerServiceState:
        """å‡çº§ç®¡ç†"""
        agent_assignment = state.get("agent_assignment", {})
        auto_resolution = state.get("auto_resolution", {})
        priority = state.get("priority", "normal")
        
        escalation_info = {
            "needs_escalation": False,
            "escalation_reason": "",
            "escalation_level": "",
            "escalation_target": "",
            "escalation_automated": False
        }
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦å‡çº§
        if not auto_resolution.get("auto_resolved", False) and priority == "urgent":
            escalation_info.update({
                "needs_escalation": True,
                "escalation_reason": "urgent_inquiry_auto_failed",
                "escalation_level": "high_priority",
                "escalation_target": "supervisor",
                "escalation_automated": True
            })
        elif agent_assignment.get("estimated_wait_time", 0) > 300:  # ç­‰å¾…æ—¶é—´è¶…è¿‡5åˆ†é’Ÿ
            escalation_info.update({
                "needs_escalation": True,
                "escalation_reason": "long_wait_time",
                "escalation_level": "resource_reallocation",
                "escalation_target": "resource_manager",
                "escalation_automated": True
            })
        
        return {
            "escalation_info": escalation_info
        }
    
    def service_quality_monitoring(state: CustomerServiceState) -> CustomerServiceState:
        """æœåŠ¡è´¨é‡ç›‘æ§"""
        session_id = state.get("session_id", "")
        agent_assignment = state.get("agent_assignment", {})
        auto_resolution = state.get("auto_resolution", {})
        escalation_info = state.get("escalation_info", {})
        
        # è®¡ç®—æœåŠ¡æŒ‡æ ‡
        service_metrics = {
            "session_id": session_id,
            "response_time": auto_resolution.get("resolution_time", 0),
            "first_contact_resolution": auto_resolution.get("auto_resolved", False),
            "agent_wait_time": agent_assignment.get("estimated_wait_time", 0),
            "escalation_count": 1 if escalation_info.get("needs_escalation", False) else 0,
            "channel_compliance": True,
            "sla_met": True,
            "customer_effort_score": calculate_customer_effort_score(auto_resolution, escalation_info)
        }
        
        # æ»¡æ„åº¦é¢„æµ‹
        satisfaction_score = predict_satisfaction_score(service_metrics)
        
        return {
            "service_metrics": service_metrics,
            "satisfaction_score": satisfaction_score
        }
    
    def calculate_customer_effort_score(auto_resolution: Dict[str, Any], 
                                        escalation: Dict[str, Any]) -> float:
        """è®¡ç®—å®¢æˆ·è´¹åŠ›æŒ‡æ•°"""
        base_score = 3.0  # ä¸­ç­‰è´¹åŠ›ç¨‹åº¦
        
        # è‡ªåŠ¨è§£å†³é™ä½è´¹åŠ›ç¨‹åº¦
        if auto_resolution.get("auto_resolved", False):
            base_score -= 1.5
        else:
            base_score += 0.5
        
        # å‡çº§å¢åŠ è´¹åŠ›ç¨‹åº¦
        if escalation.get("needs_escalation", False):
            base_score += 1.0
        
        return max(1.0, min(5.0, base_score))
    
    def predict_satisfaction_score(metrics: Dict[str, Any]) -> float:
        """é¢„æµ‹æ»¡æ„åº¦åˆ†æ•°"""
        score = 4.0  # åŸºç¡€åˆ†æ•°
        
        # ç¬¬ä¸€æ—¶é—´è§£å†³åŠ åˆ†
        if metrics.get("first_contact_resolution", False):
            score += 0.5
        
        # å“åº”æ—¶é—´å½±å“
        response_time = metrics.get("response_time", 0)
        if response_time < 60:  # 1åˆ†é’Ÿå†…
            score += 0.3
        elif response_time > 300:  # è¶…è¿‡5åˆ†é’Ÿ
            score -= 0.3
        
        # å®¢æˆ·è´¹åŠ›æŒ‡æ•°å½±å“
        effort_score = metrics.get("customer_effort_score", 3.0)
        if effort_score <= 2.0:
            score += 0.2
        elif effort_score >= 4.0:
            score -= 0.2
        
        # å‡çº§å½±å“
        if metrics.get("escalation_count", 0) > 0:
            score -= 0.4
        
        return max(1.0, min(5.0, score))
    
    # æ„å»ºå®¢æœå¹³å°å·¥ä½œæµ
    def build_customer_service_workflow():
        workflow = StateGraph(CustomerServiceState)
        
        workflow.add_node("initialize_session", initialize_service_session)
        workflow.add_node("intelligent_routing", intelligent_routing)
        workflow.add_node("knowledge_search", knowledge_base_search)
        workflow.add_node("auto_resolution", auto_resolution)
        workflow.add_node("escalation_management", escalation_management)
        workflow.add_node("quality_monitoring", service_quality_monitoring)
        
        workflow.set_entry_point("initialize_session")
        workflow.add_edge("initialize_session", "intelligent_routing")
        workflow.add_edge("intelligent_routing", "knowledge_search")
        
        # å¹¶è¡Œæ‰§è¡Œè‡ªåŠ¨è§£å†³å’Œå‡†å¤‡äººå·¥æœåŠ¡
        workflow.add_edge("knowledge_search", "auto_resolution")
        
        workflow.add_edge("auto_resolution", "escalation_management")
        workflow.add_edge("escalation_management", "quality_monitoring")
        workflow.add_edge("quality_monitoring", END)
        
        return workflow.compile()
    
    # æµ‹è¯•å®¢æœå¹³å°
    def test_customer_service_platform():
        print_step("æµ‹è¯•æ™ºèƒ½å®¢æœå¹³å°")
        
        app = build_customer_service_workflow()
        
        # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„å®¢æœè¯·æ±‚
        test_cases = [
            {
                "name": "VIPå®¢æˆ·æŠ€æœ¯é—®é¢˜",
                "customer_id": "vip_001",
                "channel": "phone",
                "inquiry_type": "technical",
                "priority": "urgent"
            },
            {
                "name": "æ™®é€šå®¢æˆ·è´¦å•å’¨è¯¢",
                "customer_id": "cust_002",
                "channel": "web",
                "inquiry_type": "billing",
                "priority": "normal"
            },
            {
                "name": "æ–°ç”¨æˆ·äº§å“å’¨è¯¢",
                "customer_id": "new_003",
                "channel": "wechat",
                "inquiry_type": "general",
                "priority": "low"
            }
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n{'='*50}")
            print(f"æµ‹è¯•æ¡ˆä¾‹ {i}: {test_case['name']}")
            print(f"{'='*50}")
            
            initial_state = {
                "session_id": f"session_{int(time.time())}_{i}",
                "customer_id": test_case["customer_id"],
                "channel": test_case["channel"],
                "inquiry_type": test_case["inquiry_type"],
                "priority": test_case["priority"],
                "customer_profile": {},
                "conversation_history": [],
                "knowledge_search_results": [],
                "agent_assignment": {},
                "auto_resolution": {},
                "escalation_info": {},
                "service_metrics": {},
                "satisfaction_score": 0.0
            }
            
            result = app.invoke(initial_state)
            
            # æ˜¾ç¤ºç»“æœ
            agent_assignment = result.get("agent_assignment", {})
            auto_resolution = result.get("auto_resolution", {})
            escalation_info = result.get("escalation_info", {})
            service_metrics = result.get("service_metrics", {})
            satisfaction_score = result.get("satisfaction_score", 0.0)
            
            print(f"\nğŸ“‹ æœåŠ¡ç»“æœ:")
            print(f"  åˆ†é…åå¸­: {agent_assignment.get('agent_id', 'N/A')}")
            print(f"  åå¸­ç­–ç•¥: {agent_assignment.get('strategy', 'N/A')}")
            print(f"  é¢„è®¡ç­‰å¾…: {agent_assignment.get('estimated_wait_time', 0)}ç§’")
            
            print(f"\nğŸ¤– è‡ªåŠ¨å¤„ç†:")
            auto_resolved = auto_resolution.get("auto_resolved", False)
            print(f"  è‡ªåŠ¨è§£å†³: {'æ˜¯' if auto_resolved else 'å¦'}")
            print(f"  åŸå› : {auto_resolution.get('reason', 'N/A')}")
            
            print(f"\nâš¡ å‡çº§ç®¡ç†:")
            needs_escalation = escalation_info.get("needs_escalation", False)
            print(f"  éœ€è¦å‡çº§: {'æ˜¯' if needs_escalation else 'å¦'}")
            if needs_escalation:
                print(f"  å‡çº§åŸå› : {escalation_info.get('escalation_reason', 'N/A')}")
            
            print(f"\nğŸ“Š æœåŠ¡è´¨é‡:")
            print(f"  å“åº”æ—¶é—´: {service_metrics.get('response_time', 0):.1f}ç§’")
            print(f"  é¦–æ¬¡è§£å†³: {'æ˜¯' if service_metrics.get('first_contact_resolution', False) else 'å¦'}")
            print(f"  å®¢æˆ·è´¹åŠ›æŒ‡æ•°: {service_metrics.get('customer_effort_score', 0):.1f}")
            print(f"  é¢„æµ‹æ»¡æ„åº¦: {satisfaction_score:.1f}/5.0")
    
    return test_customer_service_platform


# ================================
 é¡¹ç›® 2: æ•°æ®åˆ†æå¹³å°
# ================================

def project_2_data_analytics_platform():
    """
    é¡¹ç›® 2: æ™ºèƒ½æ•°æ®åˆ†æå¹³å°
    
    åŠŸèƒ½è¦æ±‚:
    1. å¤šæ•°æ®æºé›†æˆ
    2. è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—
    3. æ™ºèƒ½æ•°æ®å¯è§†åŒ–
    4. é¢„æµ‹æ€§åˆ†æ
    5. æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
    6. å¼‚å¸¸æ£€æµ‹
    7. å®æ—¶ç›‘æ§é¢æ¿
    8. æ•°æ®æ²»ç†
    
    æŠ€æœ¯æŒ‘æˆ˜:
    - å¤§æ•°æ®å¤„ç†
    - å¤æ‚ç®—æ³•é›†æˆ
    - å®æ—¶è®¡ç®—
    - å¯è§†åŒ–æ¸²æŸ“
    """
    
    class AnalyticsState(TypedDict):
        project_id: str
        data_sources: List[Dict[str, Any]]
        raw_data: Dict[str, Any]
        cleaned_data: Dict[str, Any]
        analysis_results: Dict[str, Any]
        visualizations: List[Dict[str, Any]]
        predictions: Dict[str, Any]
        anomalies: List[Dict[str, Any]]
        reports: List[Dict[str, Any]]
        quality_metrics: Dict[str, Any]
        execution_summary: Dict[str, Any]
    
    def initialize_analytics_project(state: AnalyticsState) -> AnalyticsState:
        """åˆå§‹åŒ–åˆ†æé¡¹ç›®"""
        project_id = state.get("project_id", "")
        data_sources = state.get("data_sources", [])
        
        # éªŒè¯æ•°æ®æº
        validated_sources = validate_data_sources(data_sources)
        
        # åŠ è½½åŸå§‹æ•°æ®
        raw_data = load_data_from_sources(validated_sources)
        
        return {
            "data_sources": validated_sources,
            "raw_data": raw_data
        }
    
    def validate_data_sources(sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éªŒè¯æ•°æ®æº"""
        validated_sources = []
        
        for source in sources:
            # æ¨¡æ‹Ÿæ•°æ®æºéªŒè¯
            validation_result = {
                **source,
                "validated": True,
                "validation_timestamp": time.time(),
                "accessibility": random.choice(["accessible", "restricted", "unavailable"]),
                "data_quality_score": random.uniform(0.6, 0.95),
                "estimated_size_mb": random.randint(100, 10000)
            }
            validated_sources.append(validation_result)
        
        return validated_sources
    
    def load_data_from_sources(sources: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä»æ•°æ®æºåŠ è½½æ•°æ®"""
        raw_data = {}
        
        for source in sources:
            if source.get("accessibility") == "accessible":
                # æ¨¡æ‹Ÿæ•°æ®åŠ è½½
                data_size = source.get("estimated_size_mb", 100)
                record_count = data_size * 1000  # å‡è®¾æ¯æ¡è®°å½•1KB
                
                source_data = {
                    "source_id": source.get("source_id", ""),
                    "record_count": record_count,
                    "columns": [
                        "id", "timestamp", "value", "category", "region", 
                        "user_id", "action", "amount", "status"
                    ],
                    "sample_data": generate_sample_data(record_count // 100),  # 1%æ ·æœ¬
                    "metadata": {
                        "load_time": time.time(),
                        "file_format": random.choice(["csv", "json", "parquet"]),
                        "encoding": "utf-8",
                        "compression": random.choice(["none", "gzip", "snappy"])
                    }
                }
                raw_data[source.get("source_id", "")] = source_data
        
        return raw_data
    
    def generate_sample_data(count: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ ·æœ¬æ•°æ®"""
        data = []
        
        for i in range(count):
            record = {
                "id": i,
                "timestamp": time.time() - random.uniform(0, 365*24*3600),
                "value": random.uniform(0, 1000),
                "category": random.choice(["A", "B", "C", "D"]),
                "region": random.choice(["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·"]),
                "user_id": random.randint(1000, 9999),
                "action": random.choice(["view", "click", "purchase", "cancel"]),
                "amount": random.uniform(10, 500),
                "status": random.choice(["active", "inactive", "pending"])
            }
            data.append(record)
        
        return data
    
    def data_cleaning(state: AnalyticsState) -> AnalyticsState:
        """æ•°æ®æ¸…æ´—"""
        raw_data = state.get("raw_data", {})
        
        cleaned_data = {}
        quality_metrics = {
            "total_records_before": 0,
            "total_records_after": 0,
            "duplicates_removed": 0,
            "missing_values_handled": 0,
            "outliers_detected": 0,
            "data_quality_score": 0.0
        }
        
        for source_id, source_data in raw_data.items():
            # æ‰§è¡Œæ•°æ®æ¸…æ´—
            cleaning_result = clean_source_data(source_data)
            cleaned_data[source_id] = cleaning_result["cleaned_data"]
            
            # æ›´æ–°è´¨é‡æŒ‡æ ‡
            quality_metrics["total_records_before"] += source_data.get("record_count", 0)
            quality_metrics["total_records_after"] += cleaning_result["record_count"]
            quality_metrics["duplicates_removed"] += cleaning_result["duplicates_removed"]
            quality_metrics["missing_values_handled"] += cleaning_result["missing_values_handled"]
            quality_metrics["outliers_detected"] += cleaning_result["outliers_detected"]
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        if quality_metrics["total_records_before"] > 0:
            quality_metrics["data_quality_score"] = (
                quality_metrics["total_records_after"] / quality_metrics["total_records_before"]
            )
        
        return {
            "cleaned_data": cleaned_data,
            "quality_metrics": quality_metrics
        }
    
    def clean_source_data(source_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—å•ä¸ªæ•°æ®æº"""
        sample_data = source_data.get("sample_data", [])
        original_count = len(sample_data)
        
        # å»é‡
        unique_data = []
        seen_ids = set()
        for record in sample_data:
            record_id = record.get("id")
            if record_id not in seen_ids:
                unique_data.append(record)
                seen_ids.add(record_id)
        
        duplicates_removed = original_count - len(unique_data)
        
        # å¤„ç†ç¼ºå¤±å€¼
        cleaned_data = []
        missing_values_handled = 0
        
        for record in unique_data:
            cleaned_record = record.copy()
            
            # å¡«å……ç¼ºå¤±å€¼
            for key, value in record.items():
                if value is None or value == "":
                    if key == "value":
                        cleaned_record[key] = 0.0
                    elif key == "category":
                        cleaned_record[key] = "Unknown"
                    elif key == "region":
                        cleaned_record[key] = "Unknown"
                    else:
                        cleaned_record[key] = 0
                    missing_values_handled += 1
            
            cleaned_data.append(cleaned_record)
        
        # æ£€æµ‹å¼‚å¸¸å€¼
        outliers_detected = 0
        if cleaned_data:
            values = [record.get("value", 0) for record in cleaned_data]
            if values:
                q75 = sorted(values)[int(len(values) * 0.75)]
                q25 = sorted(values)[int(len(values) * 0.25)]
                iqr = q75 - q25
                
                upper_bound = q75 + 1.5 * iqr
                lower_bound = q25 - 1.5 * iqr
                
                outliers = [v for v in values if v > upper_bound or v < lower_bound]
                outliers_detected = len(outliers)
        
        return {
            "cleaned_data": cleaned_data,
            "record_count": len(cleaned_data),
            "duplicates_removed": duplicates_removed,
            "missing_values_handled": missing_values_handled,
            "outliers_detected": outliers_detected
        }
    
    def exploratory_analysis(state: AnalyticsState) -> AnalyticsState:
        """æ¢ç´¢æ€§æ•°æ®åˆ†æ"""
        cleaned_data = state.get("cleaned_data", {})
        
        analysis_results = {
            "descriptive_statistics": {},
            "correlation_analysis": {},
            "distribution_analysis": {},
            "trend_analysis": {},
            "summary_insights": []
        }
        
        for source_id, data in cleaned_data.items():
            sample_data = data.get("cleaned_data", [])
            
            if not sample_data:
                continue
            
            # æè¿°æ€§ç»Ÿè®¡
            stats = calculate_descriptive_statistics(sample_data)
            analysis_results["descriptive_statistics"][source_id] = stats
            
            # ç›¸å…³æ€§åˆ†æ
            correlations = calculate_correlations(sample_data)
            analysis_results["correlation_analysis"][source_id] = correlations
            
            # åˆ†å¸ƒåˆ†æ
            distributions = analyze_distributions(sample_data)
            analysis_results["distribution_analysis"][source_id] = distributions
            
            # è¶‹åŠ¿åˆ†æ
            trends = analyze_trends(sample_data)
            analysis_results["trend_analysis"][source_id] = trends
        
        # ç”Ÿæˆæ´å¯Ÿ
        analysis_results["summary_insights"] = generate_summary_insights(analysis_results)
        
        return {
            "analysis_results": analysis_results
        }
    
    def calculate_descriptive_statistics(data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æè¿°æ€§ç»Ÿè®¡"""
        values = [record.get("value", 0) for record in data]
        amounts = [record.get("amount", 0) for record in data]
        
        stats = {}
        
        if values:
            stats["value"] = {
                "count": len(values),
                "mean": sum(values) / len(values),
                "median": sorted(values)[len(values) // 2],
                "min": min(values),
                "max": max(values),
                "std": calculate_std(values)
            }
        
        if amounts:
            stats["amount"] = {
                "count": len(amounts),
                "mean": sum(amounts) / len(amounts),
                "median": sorted(amounts)[len(amounts) // 2],
                "min": min(amounts),
                "max": max(amounts),
                "std": calculate_std(amounts)
            }
        
        # åˆ†ç±»ç»Ÿè®¡
        categories = [record.get("category", "") for record in data]
        category_counts = {}
        for cat in categories:
            category_counts[cat] = category_counts.get(cat, 0) + 1
        stats["category_distribution"] = category_counts
        
        # åœ°åŒºç»Ÿè®¡
        regions = [record.get("region", "") for record in data]
        region_counts = {}
        for region in regions:
            region_counts[region] = region_counts.get(region, 0) + 1
        stats["region_distribution"] = region_counts
        
        return stats
    
    def calculate_std(values: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
    
    def calculate_correlations(data: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—ç›¸å…³æ€§"""
        # ç®€åŒ–ï¼šåªè®¡ç®—æ•°å€¼å­—æ®µçš„ç›¸å…³æ€§
        values = [record.get("value", 0) for record in data]
        amounts = [record.get("amount", 0) for record in data]
        
        if len(values) != len(amounts) or len(values) < 2:
            return {}
        
        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        n = len(values)
        sum_x = sum(values)
        sum_y = sum(amounts)
        sum_xy = sum(v * a for v, a in zip(values, amounts))
        sum_x2 = sum(v ** 2 for v in values)
        sum_y2 = sum(a ** 2 for a in amounts)
        
        numerator = n * sum_xy - sum_x * sum_y
        denominator = ((n * sum_x2 - sum_x ** 2) * (n * sum_y2 - sum_y ** 2)) ** 0.5
        
        if denominator == 0:
            correlation = 0.0
        else:
            correlation = numerator / denominator
        
        return {"value_amount_correlation": correlation}
    
    def analyze_distributions(data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®åˆ†å¸ƒ"""
        values = [record.get("value", 0) for record in data]
        
        if not values:
            return {}
        
        # ç®€å•çš„åˆ†å¸ƒåˆ†æ
        sorted_values = sorted(values)
        n = len(values)
        
        distribution_analysis = {
            "quartiles": {
                "q1": sorted_values[n // 4],
                "q2": sorted_values[n // 2],  # median
                "q3": sorted_values[3 * n // 4]
            },
            "percentiles": {
                "p10": sorted_values[n // 10],
                "p90": sorted_values[9 * n // 10],
                "p95": sorted_values[95 * n // 100]
            },
            "skewness": calculate_skewness(values),
            "distribution_type": identify_distribution_type(values)
        }
        
        return distribution_analysis
    
    def calculate_skewness(values: List[float]) -> float:
        """è®¡ç®—ååº¦"""
        if len(values) < 3:
            return 0.0
        
        mean = sum(values) / len(values)
        std = calculate_std(values)
        
        if std == 0:
            return 0.0
        
        skew = sum(((x - mean) / std) ** 3 for x in values) / len(values)
        return skew
    
    def identify_distribution_type(values: List[float]) -> str:
        """è¯†åˆ«åˆ†å¸ƒç±»å‹"""
        skewness = calculate_skewness(values)
        
        if abs(skewness) < 0.5:
            return "normal"
        elif skewness > 0.5:
            return "right_skewed"
        elif skewness < -0.5:
            return "left_skewed"
        else:
            return "unknown"
    
    def analyze_trends(data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè¶‹åŠ¿"""
        if not data:
            return {}
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_data = sorted(data, key=lambda x: x.get("timestamp", 0))
        
        # ç®€å•çš„è¶‹åŠ¿åˆ†æ
        timestamps = [record.get("timestamp", 0) for record in sorted_data]
        values = [record.get("value", 0) for record in sorted_data]
        
        if len(values) < 2:
            return {}
        
        # è®¡ç®—ç®€å•çº¿æ€§è¶‹åŠ¿
        n = len(values)
        x_mean = (timestamps[-1] + timestamps[0]) / 2
        y_mean = sum(values) / n
        
        numerator = sum((t - x_mean) * (v - y_mean) for t, v in zip(timestamps, values))
        denominator = sum((t - x_mean) ** 2 for t in timestamps)
        
        if denominator == 0:
            trend_slope = 0.0
        else:
            trend_slope = numerator / denominator
        
        # è¶‹åŠ¿æ–¹å‘
        if abs(trend_slope) < 1e-10:
            trend_direction = "stable"
        elif trend_slope > 0:
            trend_direction = "increasing"
        else:
            trend_direction = "decreasing"
        
        return {
            "trend_slope": trend_slope,
            "trend_direction": trend_direction,
            "time_span": timestamps[-1] - timestamps[0],
            "data_points": n
        }
    
    def generate_summary_insights(analysis_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ´å¯Ÿæ‘˜è¦"""
        insights = []
        
        # ä»æè¿°æ€§ç»Ÿè®¡ä¸­ç”Ÿæˆæ´å¯Ÿ
        desc_stats = analysis_results.get("descriptive_statistics", {})
        for source_id, stats in desc_stats.items():
            if "value" in stats:
                value_stats = stats["value"]
                if value_stats.get("std", 0) > value_stats.get("mean", 0):
                    insights.append(f"{source_id}: æ•°å€¼å˜å¼‚æ€§è¾ƒé«˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")
            
            if "category_distribution" in stats:
                cat_dist = stats["category_distribution"]
                if cat_dist:
                    most_common = max(cat_dist.items(), key=lambda x: x[1])
                    insights.append(f"{source_id}: æœ€å¸¸è§ç±»åˆ«æ˜¯ '{most_common[0]}'ï¼Œå æ¯” {most_common[1]/sum(cat_dist.values()):.1%}")
        
        # ä»ç›¸å…³æ€§åˆ†æä¸­ç”Ÿæˆæ´å¯Ÿ
        correlations = analysis_results.get("correlation_analysis", {})
        for source_id, corr in correlations.items():
            value_amount_corr = corr.get("value_amount_correlation", 0)
            if abs(value_amount_corr) > 0.7:
                if value_amount_corr > 0:
                    insights.append(f"{source_id}: æ•°å€¼å’Œé‡‘é¢å‘ˆå¼ºæ­£ç›¸å…³")
                else:
                    insights.append(f"{source_id}: æ•°å€¼å’Œé‡‘é¢å‘ˆå¼ºè´Ÿç›¸å…³")
        
        # ä»è¶‹åŠ¿åˆ†æä¸­ç”Ÿæˆæ´å¯Ÿ
        trends = analysis_results.get("trend_analysis", {})
        for source_id, trend in trends.items():
            trend_dir = trend.get("trend_direction", "stable")
            if trend_dir == "increasing":
                insights.append(f"{source_id}: å‘ˆä¸Šå‡è¶‹åŠ¿")
            elif trend_dir == "decreasing":
                insights.append(f"{source_id}: å‘ˆä¸‹é™è¶‹åŠ¿")
        
        return insights
    
    def predictive_analysis(state: AnalyticsState) -> AnalyticsState:
        """é¢„æµ‹æ€§åˆ†æ"""
        cleaned_data = state.get("cleaned_data", {})
        
        predictions = {
            "forecasting": {},
            "classification": {},
            "anomaly_prediction": {},
            "confidence_scores": {}
        }
        
        for source_id, data in cleaned_data.items():
            sample_data = data.get("cleaned_data", [])
            
            if not sample_data:
                continue
            
            # æ—¶é—´åºåˆ—é¢„æµ‹
            forecast = time_series_forecast(sample_data)
            predictions["forecasting"][source_id] = forecast
            
            # åˆ†ç±»é¢„æµ‹
            classification = predict_categories(sample_data)
            predictions["classification"][source_id] = classification
            
            # å¼‚å¸¸é¢„æµ‹
            anomaly_pred = predict_anomalies(sample_data)
            predictions["anomaly_prediction"][source_id] = anomaly_pred
        
        return {
            "predictions": predictions
        }
    
    def time_series_forecast(data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ—¶é—´åºåˆ—é¢„æµ‹"""
        # ç®€åŒ–çš„ç§»åŠ¨å¹³å‡é¢„æµ‹
        if len(data) < 5:
            return {"error": "Insufficient data for forecasting"}
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_data = sorted(data, key=lambda x: x.get("timestamp", 0))
        values = [record.get("value", 0) for record in sorted_data]
        
        # ç®€å•ç§»åŠ¨å¹³å‡
        window_size = min(5, len(values) // 3)
        recent_values = values[-window_size:]
        forecast_value = sum(recent_values) / len(recent_values)
        
        # è®¡ç®—è¶‹åŠ¿
        if len(values) >= 10:
            early_avg = sum(values[:len(values)//2]) / (len(values)//2)
            recent_avg = sum(values[len(values)//2:]) / (len(values) - len(values)//2)
            trend = recent_avg - early_avg
        else:
            trend = 0.0
        
        # é¢„æµ‹æœªæ¥5ä¸ªç‚¹
        forecast_points = []
        for i in range(5):
            future_value = forecast_value + trend * (i + 1)
            forecast_points.append(max(0, future_value))  # ç¡®ä¿éè´Ÿ
        
        return {
            "forecast_values": forecast_points,
            "confidence_interval": 0.8,  # ç®€åŒ–çš„ç½®ä¿¡åº¦
            "trend": "increasing" if trend > 0 else "decreasing" if trend < 0 else "stable",
            "method": "moving_average_with_trend"
        }
    
    def predict_categories(data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é¢„æµ‹åˆ†ç±»"""
        # åŸºäºå†å²é¢‘ç‡çš„ç®€å•åˆ†ç±»é¢„æµ‹
        categories = [record.get("category", "") for record in data]
        category_counts = {}
        
        for cat in categories:
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        total = len(categories)
        category_probabilities = {cat: count / total for cat, count in category_counts.items()}
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„ç±»åˆ«
        predicted_category = max(category_probabilities.items(), key=lambda x: x[1])[0]
        confidence = category_probabilities[predicted_category]
        
        return {
            "predicted_category": predicted_category,
            "confidence": confidence,
            "all_probabilities": category_probabilities
        }
    
    def predict_anomalies(data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é¢„æµ‹å¼‚å¸¸"""
        values = [record.get("value", 0) for record in data]
        
        if len(values) < 10:
            return {"error": "Insufficient data for anomaly prediction"}
        
        # ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å¼‚å¸¸
        mean = sum(values) / len(values)
        std = calculate_std(values)
        
        # é¢„æµ‹å¼‚å¸¸é˜ˆå€¼
        upper_threshold = mean + 2 * std
        lower_threshold = mean - 2 * std
        
        # è¯†åˆ«å½“å‰å¼‚å¸¸
        anomalies = []
        for i, value in enumerate(values):
            if value > upper_threshold or value < lower_threshold:
                anomalies.append({
                    "index": i,
                    "value": value,
                    "anomaly_type": "high" if value > upper_threshold else "low",
                    "severity": abs(value - mean) / std
                })
        
        return {
            "anomaly_count": len(anomalies),
            "anomaly_rate": len(anomalies) / len(values),
            "thresholds": {
                "upper": upper_threshold,
                "lower": lower_threshold
            },
            "anomalies": anomalies[:5]  # åªè¿”å›å‰5ä¸ªå¼‚å¸¸
        }
    
    def generate_visualizations(state: AnalyticsState) -> AnalyticsState:
        """ç”Ÿæˆå¯è§†åŒ–"""
        analysis_results = state.get("analysis_results", {})
        predictions = state.get("predictions", {})
        
        visualizations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå¯è§†åŒ–é…ç½®
        desc_stats = analysis_results.get("descriptive_statistics", {})
        for source_id, stats in desc_stats.items():
            # æŸ±çŠ¶å›¾
            if "category_distribution" in stats:
                visualizations.append({
                    "type": "bar_chart",
                    "title": f"{source_id} Category Distribution",
                    "data": stats["category_distribution"],
                    "config": {
                        "x_axis": "Category",
                        "y_axis": "Count"
                    }
                })
            
            # ç®±çº¿å›¾
            if "value" in stats:
                visualizations.append({
                    "type": "box_plot",
                    "title": f"{source_id} Value Distribution",
                    "data": {
                        "mean": stats["value"]["mean"],
                        "median": stats["value"]["median"],
                        "q1": stats["value"]["mean"] - stats["value"]["std"],
                        "q3": stats["value"]["mean"] + stats["value"]["std"],
                        "min": stats["value"]["min"],
                        "max": stats["value"]["max"]
                    }
                })
        
        # åŸºäºé¢„æµ‹ç»“æœç”Ÿæˆå¯è§†åŒ–
        forecasting = predictions.get("forecasting", {})
        for source_id, forecast in forecasting.items():
            if "forecast_values" in forecast:
                visualizations.append({
                    "type": "line_chart",
                    "title": f"{source_id} Forecast",
                    "data": {
                        "forecast": forecast["forecast_values"],
                        "confidence": forecast.get("confidence_interval", 0.8)
                    }
                })
        
        return {
            "visualizations": visualizations
        }
    
    def generate_reports(state: AnalyticsState) -> AnalyticsState:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        analysis_results = state.get("analysis_results", {})
        predictions = state.get("predictions", {})
        quality_metrics = state.get("quality_metrics", {})
        visualizations = state.get("visualizations", [])
        
        reports = []
        
        # æ‰§è¡Œæ‘˜è¦æŠ¥å‘Š
        executive_summary = {
            "report_id": f"exec_summary_{int(time.time())}",
            "type": "executive_summary",
            "title": "æ•°æ®åˆ†ææ‰§è¡Œæ‘˜è¦",
            "content": generate_executive_summary_content(analysis_results, quality_metrics),
            "generated_at": datetime.now().isoformat(),
            "audience": "executives"
        }
        reports.append(executive_summary)
        
        # æŠ€æœ¯æŠ¥å‘Š
        technical_report = {
            "report_id": f"technical_{int(time.time())}",
            "type": "technical_report",
            "title": "è¯¦ç»†æŠ€æœ¯åˆ†ææŠ¥å‘Š",
            "content": generate_technical_report_content(analysis_results, predictions),
            "generated_at": datetime.now().isoformat(),
            "audience": "analysts"
        }
        reports.append(technical_report)
        
        # å¯è§†åŒ–æŠ¥å‘Š
        viz_report = {
            "report_id": f"visualization_{int(time.time())}",
            "type": "visualization_report",
            "title": "æ•°æ®å¯è§†åŒ–æŠ¥å‘Š",
            "content": generate_visualization_report_content(visualizations),
            "generated_at": datetime.now().isoformat(),
            "audience": "all"
        }
        reports.append(viz_report)
        
        return {
            "reports": reports
        }
    
    def generate_executive_summary_content(analysis_results: Dict[str, Any], 
                                          quality_metrics: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦å†…å®¹"""
        total_records_before = quality_metrics.get("total_records_before", 0)
        total_records_after = quality_metrics.get("total_records_after", 0)
        data_quality_score = quality_metrics.get("data_quality_score", 0.0)
        
        insights = analysis_results.get("summary_insights", [])
        
        summary = f"""
æ•°æ®åˆ†ææ‰§è¡Œæ‘˜è¦

æ•°æ®æ¦‚è§ˆ:
- å¤„ç†å‰æ€»è®°å½•æ•°: {total_records_before:,}
- æ¸…æ´—åæ€»è®°å½•æ•°: {total_records_after:,}
- æ•°æ®è´¨é‡è¯„åˆ†: {data_quality_score:.2%}

å…³é”®æ´å¯Ÿ:
"""
        
        for i, insight in enumerate(insights[:5], 1):
            summary += f"{i}. {insight}\n"
        
        summary += f"""
å»ºè®®:
- ç»§ç»­ç›‘æ§æ•°æ®è´¨é‡
- å…³æ³¨å…³é”®è¶‹åŠ¿å˜åŒ–
- æ·±å…¥åˆ†æå¼‚å¸¸æ¨¡å¼
"""
        
        return summary
    
    def generate_technical_report_content(analysis_results: Dict[str, Any], 
                                         predictions: Dict[str, Any]) -> str:
        """ç”ŸæˆæŠ€æœ¯æŠ¥å‘Šå†…å®¹"""
        return f"""
è¯¦ç»†æŠ€æœ¯åˆ†ææŠ¥å‘Š

åˆ†ææ–¹æ³•:
- æè¿°æ€§ç»Ÿè®¡åˆ†æ
- ç›¸å…³æ€§åˆ†æ
- åˆ†å¸ƒåˆ†æ
- è¶‹åŠ¿åˆ†æ
- é¢„æµ‹å»ºæ¨¡

ä¸»è¦å‘ç°:
{json.dumps(analysis_results, indent=2, ensure_ascii=False)[:1000]}...

é¢„æµ‹ç»“æœ:
{json.dumps(predictions, indent=2, ensure_ascii=False)[:1000]}...

æŠ€æœ¯å»ºè®®:
- è€ƒè™‘ä½¿ç”¨æ›´é«˜çº§çš„é¢„æµ‹æ¨¡å‹
- å¢åŠ ç‰¹å¾å·¥ç¨‹
- ä¼˜åŒ–æ•°æ®æ¸…æ´—æµç¨‹
"""
    
    def generate_visualization_report_content(visualizations: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šå†…å®¹"""
        return f"""
æ•°æ®å¯è§†åŒ–æŠ¥å‘Š

å¯è§†åŒ–æ¦‚è§ˆ:
- ç”Ÿæˆå›¾è¡¨æ•°é‡: {len(visualizations)}
- å›¾è¡¨ç±»å‹: {[viz['type'] for viz in visualizations]}

è¯¦ç»†å›¾è¡¨:
{json.dumps(visualizations, indent=2, ensure_ascii=False)[:1500]}...
"""
    
    def create_execution_summary(state: AnalyticsState) -> AnalyticsState:
        """åˆ›å»ºæ‰§è¡Œæ‘˜è¦"""
        project_id = state.get("project_id", "")
        quality_metrics = state.get("quality_metrics", {})
        analysis_results = state.get("analysis_results", {})
        predictions = state.get("predictions", {})
        visualizations = state.get("visualizations", [])
        reports = state.get("reports", [])
        
        execution_summary = {
            "project_id": project_id,
            "execution_time": time.time(),
            "data_sources_processed": len(state.get("data_sources", [])),
            "total_records_analyzed": quality_metrics.get("total_records_after", 0),
            "data_quality_score": quality_metrics.get("data_quality_score", 0.0),
            "visualizations_generated": len(visualizations),
            "reports_created": len(reports),
            "predictions_made": len(predictions.get("forecasting", {})),
            "key_insights": len(analysis_results.get("summary_insights", [])),
            "anomalies_detected": sum(
                pred.get("anomaly_count", 0) 
                for pred in predictions.get("anomaly_prediction", {}).values()
            )
        }
        
        return {
            "execution_summary": execution_summary
        }
    
    # æ„å»ºæ•°æ®åˆ†æå¹³å°å·¥ä½œæµ
    def build_analytics_workflow():
        workflow = StateGraph(AnalyticsState)
        
        workflow.add_node("initialize_project", initialize_analytics_project)
        workflow.add_node("data_cleaning", data_cleaning)
        workflow.add_node("exploratory_analysis", exploratory_analysis)
        workflow.add_node("predictive_analysis", predictive_analysis)
        workflow.add_node("generate_visualizations", generate_visualizations)
        workflow.add_node("generate_reports", generate_reports)
        workflow.add_node("create_summary", create_execution_summary)
        
        workflow.set_entry_point("initialize_project")
        workflow.add_edge("initialize_project", "data_cleaning")
        workflow.add_edge("data_cleaning", "exploratory_analysis")
        workflow.add_edge("exploratory_analysis", "predictive_analysis")
        
        # å¹¶è¡Œæ‰§è¡Œå¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ
        workflow.add_edge("predictive_analysis", "generate_visualizations")
        workflow.add_edge("predictive_analysis", "generate_reports")
        
        workflow.add_edge("generate_visualizations", "create_summary")
        workflow.add_edge("generate_reports", "create_summary")
        workflow.add_edge("create_summary", END)
        
        return workflow.compile()
    
    # æµ‹è¯•æ•°æ®åˆ†æå¹³å°
    def test_data_analytics_platform():
        print_step("æµ‹è¯•æ•°æ®åˆ†æå¹³å°")
        
        app = build_analytics_workflow()
        
        # æ¨¡æ‹Ÿæ•°æ®æº
        data_sources = [
            {
                "source_id": "sales_data",
                "source_type": "database",
                "connection_string": "postgresql://...",
                "table_name": "sales_transactions",
                "last_updated": time.time() - 86400
            },
            {
                "source_id": "user_behavior",
                "source_type": "file",
                "file_path": "/data/user_events.csv",
                "format": "csv",
                "last_updated": time.time() - 3600
            },
            {
                "source_id": "inventory",
                "source_type": "api",
                "api_endpoint": "https://api.company.com/inventory",
                "auth_required": True,
                "last_updated": time.time() - 1800
            }
        ]
        
        initial_state = {
            "project_id": f"analytics_project_{int(time.time())}",
            "data_sources": data_sources,
            "raw_data": {},
            "cleaned_data": {},
            "analysis_results": {},
            "visualizations": [],
            "predictions": {},
            "anomalies": [],
            "reports": [],
            "quality_metrics": {},
            "execution_summary": {}
        }
        
        result = app.invoke(initial_state)
        
        # æ˜¾ç¤ºç»“æœ
        quality_metrics = result.get("quality_metrics", {})
        analysis_results = result.get("analysis_results", {})
        visualizations = result.get("visualizations", [])
        reports = result.get("reports", [])
        execution_summary = result.get("execution_summary", {})
        
        print(f"\nğŸ“Š æ•°æ®è´¨é‡æŒ‡æ ‡:")
        print(f"  å¤„ç†å‰è®°å½•æ•°: {quality_metrics.get('total_records_before', 0):,}")
        print(f"  å¤„ç†åè®°å½•æ•°: {quality_metrics.get('total_records_after', 0):,}")
        print(f"  æ•°æ®è´¨é‡è¯„åˆ†: {quality_metrics.get('data_quality_score', 0):.2%}")
        print(f"  å»é‡è®°å½•æ•°: {quality_metrics.get('duplicates_removed', 0):,}")
        print(f"  å¤„ç†ç¼ºå¤±å€¼: {quality_metrics.get('missing_values_handled', 0):,}")
        
        summary_insights = analysis_results.get("summary_insights", [])
        if summary_insights:
            print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
            for i, insight in enumerate(summary_insights[:3], 1):
                print(f"  {i}. {insight}")
        
        print(f"\nğŸ“ˆ å¯è§†åŒ–ç»“æœ:")
        print(f"  ç”Ÿæˆå›¾è¡¨æ•°é‡: {len(visualizations)}")
        chart_types = [viz.get("type", "unknown") for viz in visualizations]
        for chart_type in set(chart_types):
            count = chart_types.count(chart_type)
            print(f"  - {chart_type}: {count}ä¸ª")
        
        print(f"\nğŸ“‹ åˆ†ææŠ¥å‘Š:")
        print(f"  ç”ŸæˆæŠ¥å‘Šæ•°: {len(reports)}")
        for report in reports:
            print(f"  - {report.get('title', 'Unnamed')} ({report.get('type', 'unknown')})")
        
        print(f"\nâš¡ æ‰§è¡Œæ‘˜è¦:")
        print(f"  æ•°æ®æºæ•°é‡: {execution_summary.get('data_sources_processed', 0)}")
        print(f"  åˆ†æè®°å½•æ•°: {execution_summary.get('total_records_analyzed', 0):,}")
        print(f"  æ£€æµ‹å¼‚å¸¸æ•°: {execution_summary.get('anomalies_detected', 0)}")
        print(f"  å…³é”®æ´å¯Ÿæ•°: {execution_summary.get('key_insights', 0)}")
    
    return test_data_analytics_platform


# ================================
 ä¸»æµ‹è¯•å‡½æ•°
# ================================

def run_real_projects():
    """è¿è¡ŒçœŸå®é¡¹ç›®æµ‹è¯•"""
    print("ğŸš€ LangGraph çœŸå®é¡¹ç›®å®è·µ")
    print("=" * 60)
    
    projects = [
        ("æ™ºèƒ½å®¢æœå¹³å°", project_1_customer_service_platform),
        ("æ•°æ®åˆ†æå¹³å°", project_2_data_analytics_platform)
    ]
    
    while True:
        print("\nè¯·é€‰æ‹©é¡¹ç›®:")
        for i, (name, func) in enumerate(projects, 1):
            print(f"{i}. {name}")
        print("3. è¿è¡Œæ‰€æœ‰é¡¹ç›®")
        print("0. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-3): ").strip()
        
        if choice == "1":
            projects[0][1]()
        elif choice == "2":
            projects[1][1]()
        elif choice == "3":
            print("\n" + "="*50)
            print("è¿è¡Œæ‰€æœ‰çœŸå®é¡¹ç›®")
            print("="*50)
            for name, func in projects:
                print(f"\n{'='*20} {name} {'='*20}")
                func()
                time.sleep(3)
        elif choice == "0":
            print_step("æ„Ÿè°¢å®ŒæˆçœŸå®é¡¹ç›®å®è·µï¼")
            break
        else:
            print_error("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
    
    print_result("çœŸå®é¡¹ç›®å®è·µå®Œæˆï¼")


if __name__ == "__main__":
    run_real_projects()
    
    print_step("""
çœŸå®é¡¹ç›®å®è·µå®Œæˆæ€»ç»“:

1. æ™ºèƒ½å®¢æœå¹³å°
   - å¤šæ¸ é“æ¥å…¥æ”¯æŒ
   - æ™ºèƒ½è·¯ç”±å’Œåˆ†é…
   - çŸ¥è¯†åº“é›†æˆ
   - è‡ªåŠ¨è§£å†³å’Œäººå·¥è½¬æ¥
   - æœåŠ¡è´¨é‡ç›‘æ§
   - å®¢æˆ·æ»¡æ„åº¦ç®¡ç†

2. æ•°æ®åˆ†æå¹³å°
   - å¤šæ•°æ®æºé›†æˆ
   - è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—
   - æ¢ç´¢æ€§æ•°æ®åˆ†æ
   - é¢„æµ‹æ€§åˆ†æ
   - å¯è§†åŒ–ç”Ÿæˆ
   - æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ

è¿™äº›é¡¹ç›®å±•ç¤ºäº†LangGraphåœ¨ä¼ä¸šçº§åº”ç”¨ä¸­çš„
å¼ºå¤§èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤æ‚ä¸šåŠ¡é€»è¾‘å¤„ç†ã€
å¤šç³»ç»Ÿé›†æˆã€å®æ—¶æ€§èƒ½è¦æ±‚ç­‰ã€‚

é€šè¿‡è¿™äº›é¡¹ç›®å®è·µï¼Œæ‚¨åº”è¯¥å·²ç»æŒæ¡äº†:
- å¤æ‚çŠ¶æ€ç®¡ç†
- é«˜çº§å·¥ä½œæµè®¾è®¡
- æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- é”™è¯¯å¤„ç†ç­–ç•¥
- ä¼ä¸šçº§åº”ç”¨æ¶æ„

æ­å–œæ‚¨å®ŒæˆLangGraphçš„å®Œæ•´å­¦ä¹ æ—…ç¨‹ï¼
    """)