"""
05-exercises: é«˜çº§é—®é¢˜è§£å†³

è¿™ä¸ªæ–‡ä»¶åŒ…å«LangGraphé«˜çº§é—®é¢˜çš„ç»ƒä¹ ï¼ŒæŒ‘æˆ˜æ‚¨çš„æŠ€æœ¯æ·±åº¦
å’Œè§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚

ç»ƒä¹ åŒ…æ‹¬ï¼š
- å¤æ‚çŠ¶æ€ç®¡ç†
- é«˜çº§è·¯ç”±ç­–ç•¥
- æ€§èƒ½ä¼˜åŒ–
- å®‰å…¨å’Œç›‘æ§
- é”™è¯¯æ¢å¤
"""

from typing import TypedDict, List, Dict, Any, Literal, Optional
from langgraph.graph import StateGraph, END
import sys
import os
import time
import asyncio
import random
import json
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils import print_step, print_result, print_error

# ================================
 ç»ƒä¹  1: æ™ºèƒ½æ¨èç³»ç»Ÿ
# ================================

def exercise_1_recommendation_system():
    """
    ç»ƒä¹  1: æ™ºèƒ½æ¨èç³»ç»Ÿ
    
    è¦æ±‚:
    1. å®ç°åŸºäºç”¨æˆ·è¡Œä¸ºçš„å†…å®¹æ¨è
    2. æ”¯æŒå¤šç§æ¨èç®—æ³•ï¼ˆååŒè¿‡æ»¤ã€å†…å®¹æ¨èç­‰ï¼‰
    3. åŠ¨æ€è°ƒæ•´æ¨èç­–ç•¥
    4. å®æ—¶å­¦ä¹ å’Œä¼˜åŒ–
    5. A/Bæµ‹è¯•åŠŸèƒ½
    
    æŒ‘æˆ˜ç‚¹:
    - å¤æ‚çš„ç”¨æˆ·ç”»åƒå»ºæ¨¡
    - å®æ—¶æ€§èƒ½è¦æ±‚
    - å†·å¯åŠ¨é—®é¢˜å¤„ç†
    - æ¨èå¤šæ ·æ€§æ§åˆ¶
    """
    
    # å®ç°çŠ¶æ€å®šä¹‰
    class RecommendationState(TypedDict):
        user_id: str
        request_context: Dict[str, Any]
        user_profile: Dict[str, Any]
        behavior_history: List[Dict[str, Any]]
        candidate_items: List[Dict[str, Any]]
        recommendation_strategy: str
        scored_items: List[Dict[str, Any]]
        final_recommendations: List[Dict[str, Any]]
        ab_test_group: str
        performance_metrics: Dict[str, Any]
    
    # ç”¨æˆ·ç”»åƒæ„å»º
    def build_user_profile(state: RecommendationState) -> RecommendationState:
        """æ„å»ºç”¨æˆ·ç”»åƒ"""
        user_id = state.get("user_id", "")
        behavior_history = state.get("behavior_history", [])
        
        # æ¨¡æ‹Ÿç”¨æˆ·ç”»åƒåˆ†æ
        profile = {
            "user_id": user_id,
            "interests": [],
            "preferences": {},
            "activity_level": 0,
            "last_active": None,
            "demographics": {}
        }
        
        # åˆ†æè¡Œä¸ºå†å²
        if behavior_history:
            # æå–å…´è¶£æ ‡ç­¾
            all_tags = []
            for behavior in behavior_history:
                tags = behavior.get("tags", [])
                all_tags.extend(tags)
            
            # ç»Ÿè®¡æ ‡ç­¾é¢‘ç‡
            tag_counts = {}
            for tag in all_tags:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
            
            # é€‰æ‹©é«˜é¢‘æ ‡ç­¾ä½œä¸ºå…´è¶£
            sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)
            profile["interests"] = [tag for tag, count in sorted_tags[:5]]
            
            # è®¡ç®—æ´»è·ƒåº¦
            profile["activity_level"] = len(behavior_history)
            profile["last_active"] = max(behavior["timestamp"] for behavior in behavior_history)
        
        return {"user_profile": profile}
    
    # ååŒè¿‡æ»¤æ¨è
    def collaborative_filtering(state: RecommendationState) -> RecommendationState:
        """ååŒè¿‡æ»¤ç®—æ³•"""
        user_profile = state.get("user_profile", {})
        candidate_items = state.get("candidate_items", [])
        
        # æ¨¡æ‹ŸååŒè¿‡æ»¤
        similar_users = find_similar_users(user_profile["user_id"])
        scored_items = []
        
        for item in candidate_items:
            # è®¡ç®—ååŒè¿‡æ»¤è¯„åˆ†
            cf_score = calculate_cf_score(item, similar_users)
            
            item_with_score = {
                **item,
                "cf_score": cf_score,
                "scoring_method": "collaborative_filtering"
            }
            scored_items.append(item_with_score)
        
        return {"scored_items": scored_items}
    
    # å†…å®¹æ¨è
    def content_based_recommendation(state: RecommendationState) -> RecommendationState:
        """åŸºäºå†…å®¹çš„æ¨è"""
        user_profile = state.get("user_profile", {})
        candidate_items = state.get("candidate_items", [])
        scored_items = state.get("scored_items", [])
        
        for item in scored_items:
            # è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
            content_score = calculate_content_similarity(item, user_profile)
            item["content_score"] = content_score
        
        return {"scored_items": scored_items}
    
    def find_similar_users(user_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾ç›¸ä¼¼ç”¨æˆ·"""
        # æ¨¡æ‹ŸæŸ¥æ‰¾ç›¸ä¼¼ç”¨æˆ·
        similar_users = []
        for i in range(limit):
            similar_user = {
                "user_id": f"user_{i}",
                "similarity": random.uniform(0.3, 0.9),
                "preferences": {}
            }
            similar_users.append(similar_user)
        return similar_users
    
    def calculate_cf_score(item: Dict[str, Any], similar_users: List[Dict[str, Any]]) -> float:
        """è®¡ç®—ååŒè¿‡æ»¤è¯„åˆ†"""
        base_score = random.uniform(0.1, 0.9)
        similarity_weight = sum(user["similarity"] for user in similar_users) / len(similar_users)
        return min(base_score * similarity_weight * 1.2, 1.0)
    
    def calculate_content_similarity(item: Dict[str, Any], user_profile: Dict[str, Any]) -> float:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦"""
        user_interests = user_profile.get("interests", [])
        item_tags = item.get("tags", [])
        
        # è®¡ç®—æ ‡ç­¾é‡å åº¦
        common_tags = set(user_interests) & set(item_tags)
        if not item_tags:
            return 0.1
        
        similarity = len(common_tags) / len(item_tags)
        return min(similarity * 1.1, 1.0)
    
    def choose_recommendation_strategy(state: RecommendationState) -> RecommendationState:
        """é€‰æ‹©æ¨èç­–ç•¥"""
        user_profile = state.get("user_profile", {})
        ab_test_group = random.choice(["control", "treatment"])
        
        # æ ¹æ®ç”¨æˆ·ç‰¹å¾é€‰æ‹©ç­–ç•¥
        activity_level = user_profile.get("activity_level", 0)
        interests = user_profile.get("interests", [])
        
        if activity_level < 5:  # æ–°ç”¨æˆ·
            strategy = "content_based"
        elif len(interests) > 10:  # æ´»è·ƒç”¨æˆ·
            strategy = "collaborative_filtering"
        else:  # æ··åˆç­–ç•¥
            strategy = "hybrid"
        
        return {
            "recommendation_strategy": strategy,
            "ab_test_group": ab_test_group
        }
    
    def rank_recommendations(state: RecommendationState) -> RecommendationState:
        """æ’åºæ¨èç»“æœ"""
        scored_items = state.get("scored_items", [])
        strategy = state.get("recommendation_strategy", "hybrid")
        
        # æ ¹æ®ç­–ç•¥è®¡ç®—æœ€ç»ˆåˆ†æ•°
        for item in scored_items:
            cf_score = item.get("cf_score", 0)
            content_score = item.get("content_score", 0)
            
            if strategy == "collaborative_filtering":
                final_score = cf_score * 0.8 + content_score * 0.2
            elif strategy == "content_based":
                final_score = content_score * 0.8 + cf_score * 0.2
            else:  # hybrid
                final_score = cf_score * 0.6 + content_score * 0.4
            
            item["final_score"] = final_score
        
        # æ’åºå¹¶è¿”å›å‰Nä¸ª
        sorted_items = sorted(scored_items, key=lambda x: x["final_score"], reverse=True)
        final_recommendations = sorted_items[:10]
        
        return {"final_recommendations": final_recommendations}
    
    def evaluate_performance(state: RecommendationState) -> RecommendationState:
        """è¯„ä¼°æ¨èæ€§èƒ½"""
        final_recommendations = state.get("final_recommendations", [])
        strategy = state.get("recommendation_strategy", "")
        ab_test_group = state.get("ab_test_group", "")
        
        # æ¨¡æ‹Ÿæ€§èƒ½æŒ‡æ ‡
        metrics = {
            "recommendation_count": len(final_recommendations),
            "avg_score": sum(item.get("final_score", 0) for item in final_recommendations) / len(final_recommendations),
            "diversity_score": calculate_diversity(final_recommendations),
            "coverage_score": random.uniform(0.6, 0.9),
            "response_time_ms": random.randint(50, 200),
            "strategy": strategy,
            "ab_test_group": ab_test_group
        }
        
        return {"performance_metrics": metrics}
    
    def calculate_diversity(recommendations: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ¨èå¤šæ ·æ€§"""
        if len(recommendations) < 2:
            return 0.0
        
        # ç®€å•çš„å¤šæ ·æ€§è®¡ç®—ï¼šåŸºäºæ ‡ç­¾çš„å·®å¼‚
        all_tags = []
        for rec in recommendations:
            all_tags.extend(rec.get("tags", []))
        
        unique_tags = set(all_tags)
        diversity = len(unique_tags) / len(all_tags) if all_tags else 0
        return min(diversity, 1.0)
    
    # æ„å»ºæ¨èç³»ç»Ÿå·¥ä½œæµ
    def build_recommendation_workflow():
        workflow = StateGraph(RecommendationState)
        
        workflow.add_node("build_profile", build_user_profile)
        workflow.add_node("choose_strategy", choose_recommendation_strategy)
        workflow.add_node("collaborative_filtering", collaborative_filtering)
        workflow.add_node("content_based", content_based_recommendation)
        workflow.add_node("rank_recommendations", rank_recommendations)
        workflow.add_node("evaluate_performance", evaluate_performance)
        
        workflow.set_entry_point("build_profile")
        workflow.add_edge("build_profile", "choose_strategy")
        
        # å¹¶è¡Œæ‰§è¡Œä¸¤ç§æ¨èç®—æ³•
        workflow.add_edge("choose_strategy", "collaborative_filtering")
        workflow.add_edge("choose_strategy", "content_based")
        
        workflow.add_edge("collaborative_filtering", "rank_recommendations")
        workflow.add_edge("content_based", "rank_recommendations")
        
        workflow.add_edge("rank_recommendations", "evaluate_performance")
        workflow.add_edge("evaluate_performance", END)
        
        return workflow.compile()
    
    # æµ‹è¯•å‡½æ•°
    def test_recommendation_system():
        print_step("æµ‹è¯•æ™ºèƒ½æ¨èç³»ç»Ÿ")
        
        app = build_recommendation_workflow()
        
        # æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºå†å²
        behavior_history = [
            {"item_id": "item1", "action": "view", "timestamp": time.time() - 86400, "tags": ["tech", "ai"]},
            {"item_id": "item2", "action": "like", "timestamp": time.time() - 43200, "tags": ["programming", "python"]},
            {"item_id": "item3", "action": "purchase", "timestamp": time.time() - 21600, "tags": ["education", "course"]},
        ]
        
        # æ¨¡æ‹Ÿå€™é€‰ç‰©å“
        candidate_items = [
            {"item_id": "item4", "title": "LangGraphæ•™ç¨‹", "tags": ["tech", "programming", "langgraph"]},
            {"item_id": "item5", "title": "Pythoné«˜çº§è¯¾ç¨‹", "tags": ["programming", "python", "education"]},
            {"item_id": "item6", "title": "AIå®è·µæŒ‡å—", "tags": ["tech", "ai", "programming"]},
            {"item_id": "item7", "title": "æœºå™¨å­¦ä¹ åŸºç¡€", "tags": ["tech", "ai", "education"]},
        ]
        
        initial_state = {
            "user_id": "user123",
            "request_context": {"page": "homepage", "timestamp": time.time()},
            "behavior_history": behavior_history,
            "candidate_items": candidate_items,
            "user_profile": {},
            "scored_items": [],
            "final_recommendations": [],
            "recommendation_strategy": "",
            "ab_test_group": "",
            "performance_metrics": {}
        }
        
        result = app.invoke(initial_state)
        
        # æ˜¾ç¤ºç»“æœ
        user_profile = result.get("user_profile", {})
        final_recommendations = result.get("final_recommendations", [])
        performance_metrics = result.get("performance_metrics", {})
        
        print(f"\nğŸ‘¤ ç”¨æˆ·ç”»åƒ:")
        print(f"  å…´è¶£æ ‡ç­¾: {user_profile.get('interests', [])}")
        print(f"  æ´»è·ƒåº¦: {user_profile.get('activity_level', 0)}")
        
        print(f"\nğŸ¯ æ¨èç­–ç•¥: {result.get('recommendation_strategy', '')}")
        print(f"ğŸ§ª A/Bæµ‹è¯•ç»„: {result.get('ab_test_group', '')}")
        
        print(f"\nğŸ“‹ æ¨èç»“æœ:")
        for i, rec in enumerate(final_recommendations[:5], 1):
            score = rec.get("final_score", 0)
            title = rec.get("title", rec.get("item_id", ""))
            print(f"  {i}. {title} (è¯„åˆ†: {score:.3f})")
        
        print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å¹³å‡è¯„åˆ†: {performance_metrics.get('avg_score', 0):.3f}")
        print(f"  å¤šæ ·æ€§: {performance_metrics.get('diversity_score', 0):.3f}")
        print(f"  å“åº”æ—¶é—´: {performance_metrics.get('response_time_ms', 0)}ms")
    
    return test_recommendation_system


# ================================
 ç»ƒä¹  2: å®æ—¶æ•°æ®æµå¤„ç†
# ================================

def exercise_2_stream_processing():
    """
    ç»ƒä¹  2: å®æ—¶æ•°æ®æµå¤„ç†
    
    è¦æ±‚:
    1. å¤„ç†é«˜å¹¶å‘æ•°æ®æµ
    2. å®ç°å®æ—¶èšåˆå’Œåˆ†æ
    3. æ”¯æŒåŠ¨æ€è§„åˆ™å¼•æ“
    4. å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦
    5. èƒŒå‹å¤„ç†æœºåˆ¶
    
    æŒ‘æˆ˜ç‚¹:
    - é«˜æ€§èƒ½è¦æ±‚
    - æ•°æ®ä¸€è‡´æ€§ä¿è¯
    - å†…å­˜ç®¡ç†
    - æ•…éšœæ¢å¤
    """
    
    class StreamProcessingState(TypedDict):
        stream_id: str
        data_events: List[Dict[str, Any]]
        processing_rules: List[Dict[str, Any]]
        aggregated_results: Dict[str, Any]
        alerts: List[Dict[str, Any]]
        performance_stats: Dict[str, Any]
        buffer_status: Dict[str, Any]
        error_log: List[Dict[str, Any]]
    
    class EventType(Enum):
        METRIC = "metric"
        EVENT = "event"
        LOG = "log"
        ALERT = "alert"
    
    class ProcessingPriority(Enum):
        HIGH = 1
        MEDIUM = 2
        LOW = 3
    
    @dataclass
    class DataEvent:
        event_id: str
        event_type: EventType
        timestamp: float
        data: Dict[str, Any]
        priority: ProcessingPriority
        processed: bool = False
    
    # æ•°æ®ç¼“å†²ç®¡ç†
    def manage_buffer(state: StreamProcessingState) -> StreamProcessingState:
        """ç®¡ç†æ•°æ®ç¼“å†²åŒº"""
        data_events = state.get("data_events", [])
        
        # ç¼“å†²åŒºçŠ¶æ€æ£€æŸ¥
        buffer_size = len(data_events)
        buffer_capacity = 1000  # æœ€å¤§ç¼“å†²åŒºå¤§å°
        
        buffer_status = {
            "current_size": buffer_size,
            "capacity": buffer_capacity,
            "utilization": buffer_size / buffer_capacity,
            "status": "normal"
        }
        
        # èƒŒå‹å¤„ç†
        if buffer_size > buffer_capacity * 0.8:
            buffer_status["status"] = "warning"
            # å®æ–½èƒŒå‹ç­–ç•¥
            processed_events = apply_backpressure(data_events)
            buffer_status["dropped_events"] = len(data_events) - len(processed_events)
        else:
            processed_events = data_events
        
        return {
            "data_events": processed_events,
            "buffer_status": buffer_status
        }
    
    def apply_backpressure(events: List[DataEvent]) -> List[DataEvent]:
        """åº”ç”¨èƒŒå‹å¤„ç†"""
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_events = sorted(events, key=lambda x: x.priority.value)
        
        # ä¿ç•™é«˜ä¼˜å…ˆçº§å’Œä¸­ç­‰ä¼˜å…ˆçº§çš„äº‹ä»¶
        filtered_events = [e for e in sorted_events if e.priority.value <= ProcessingPriority.MEDIUM.value]
        
        # å¦‚æœè¿˜æ˜¯å¤ªå¤šï¼Œä¿ç•™æœ€æ–°çš„äº‹ä»¶
        if len(filtered_events) > 800:
            filtered_events = filtered_events[-800:]
        
        return filtered_events
    
    # å®æ—¶èšåˆ
    def real_time_aggregation(state: StreamProcessingState) -> StreamProcessingState:
        """å®æ—¶æ•°æ®èšåˆ"""
        data_events = state.get("data_events", [])
        
        aggregated_results = {
            "total_events": len(data_events),
            "event_types": {},
            "time_window": {},
            "key_metrics": {},
            "aggregation_timestamp": datetime.now().isoformat()
        }
        
        # æŒ‰äº‹ä»¶ç±»å‹ç»Ÿè®¡
        for event in data_events:
            event_type = event.get("event_type", "unknown")
            aggregated_results["event_types"][event_type] = aggregated_results["event_types"].get(event_type, 0) + 1
        
        # æ—¶é—´çª—å£èšåˆ
        current_time = time.time()
        time_windows = {"1m": 60, "5m": 300, "1h": 3600}
        
        for window_name, window_seconds in time_windows.items():
            window_start = current_time - window_seconds
            window_events = [e for e in data_events if e.get("timestamp", 0) > window_start]
            aggregated_results["time_window"][window_name] = len(window_events)
        
        # å…³é”®æŒ‡æ ‡èšåˆ
        metric_events = [e for e in data_events if e.get("event_type") == "metric"]
        if metric_events:
            for event in metric_events:
                metric_data = event.get("data", {})
                metric_name = metric_data.get("name", "unknown")
                metric_value = metric_data.get("value", 0)
                
                if metric_name not in aggregated_results["key_metrics"]:
                    aggregated_results["key_metrics"][metric_name] = {
                        "count": 0,
                        "sum": 0,
                        "avg": 0,
                        "min": float('inf'),
                        "max": float('-inf')
                    }
                
                metrics = aggregated_results["key_metrics"][metric_name]
                metrics["count"] += 1
                metrics["sum"] += metric_value
                metrics["avg"] = metrics["sum"] / metrics["count"]
                metrics["min"] = min(metrics["min"], metric_value)
                metrics["max"] = max(metrics["max"], metric_value)
        
        return {"aggregated_results": aggregated_results}
    
    # è§„åˆ™å¼•æ“
    def apply_processing_rules(state: StreamProcessingState) -> StreamProcessingState:
        """åº”ç”¨å¤„ç†è§„åˆ™"""
        data_events = state.get("data_events", [])
        processing_rules = state.get("processing_rules", [])
        alerts = state.get("alerts", [])
        
        for event in data_events:
            for rule in processing_rules:
                if evaluate_rule(event, rule):
                    alert = create_alert(event, rule)
                    alerts.append(alert)
        
        return {"alerts": alerts}
    
    def evaluate_rule(event: Dict[str, Any], rule: Dict[str, Any]) -> bool:
        """è¯„ä¼°è§„åˆ™æ¡ä»¶"""
        conditions = rule.get("conditions", [])
        
        for condition in conditions:
            field = condition.get("field")
            operator = condition.get("operator")
            value = condition.get("value")
            
            event_value = get_nested_value(event, field)
            
            if not compare_values(event_value, operator, value):
                return False
        
        return True
    
    def get_nested_value(obj: Dict[str, Any], path: str) -> Any:
        """è·å–åµŒå¥—å­—å…¸å€¼"""
        keys = path.split(".")
        current = obj
        
        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return None
        
        return current
    
    def compare_values(actual: Any, operator: str, expected: Any) -> bool:
        """æ¯”è¾ƒå€¼"""
        try:
            if operator == ">":
                return actual > expected
            elif operator == "<":
                return actual < expected
            elif operator == ">=":
                return actual >= expected
            elif operator == "<=":
                return actual <= expected
            elif operator == "==":
                return actual == expected
            elif operator == "!=":
                return actual != expected
            elif operator == "contains":
                return expected in str(actual)
            else:
                return False
        except:
            return False
    
    def create_alert(event: Dict[str, Any], rule: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå‘Šè­¦"""
        return {
            "alert_id": f"alert_{int(time.time())}_{random.randint(1000, 9999)}",
            "event_id": event.get("event_id", ""),
            "rule_name": rule.get("name", ""),
            "severity": rule.get("severity", "medium"),
            "message": rule.get("message", "Rule triggered"),
            "event_data": event,
            "timestamp": datetime.now().isoformat()
        }
    
    # å¼‚å¸¸æ£€æµ‹
    def detect_anomalies(state: StreamProcessingState) -> StreamProcessingState:
        """å¼‚å¸¸æ£€æµ‹"""
        aggregated_results = state.get("aggregated_results", {})
        alerts = state.get("alerts", [])
        
        # åŸºäºèšåˆç»“æœçš„å¼‚å¸¸æ£€æµ‹
        key_metrics = aggregated_results.get("key_metrics", {})
        
        for metric_name, metrics in key_metrics.items():
            # æ£€æµ‹å¼‚å¸¸å€¼
            avg = metrics.get("avg", 0)
            max_val = metrics.get("max", 0)
            
            # ç®€å•çš„å¼‚å¸¸æ£€æµ‹è§„åˆ™
            if max_val > avg * 10:  # æœ€å¤§å€¼è¿œå¤§äºå¹³å‡å€¼
                alert = {
                    "alert_id": f"anomaly_{metric_name}_{int(time.time())}",
                    "type": "anomaly_detection",
                    "metric": metric_name,
                    "reason": f"Max value ({max_val}) is much higher than average ({avg})",
                    "severity": "high",
                    "timestamp": datetime.now().isoformat()
                }
                alerts.append(alert)
        
        return {"alerts": alerts}
    
    # æ€§èƒ½ç»Ÿè®¡
    def calculate_performance_stats(state: StreamProcessingState) -> StreamProcessingState:
        """è®¡ç®—æ€§èƒ½ç»Ÿè®¡"""
        data_events = state.get("data_events", [])
        aggregated_results = state.get("aggregated_results", {})
        alerts = state.get("alerts", [])
        buffer_status = state.get("buffer_status", {})
        
        start_time = time.time() - random.uniform(5, 30)  # æ¨¡æ‹Ÿå¤„ç†å¼€å§‹æ—¶é—´
        end_time = time.time()
        processing_time = end_time - start_time
        
        performance_stats = {
            "processing_time_seconds": processing_time,
            "events_per_second": len(data_events) / processing_time if processing_time > 0 else 0,
            "total_events_processed": len(data_events),
            "alerts_generated": len(alerts),
            "buffer_utilization": buffer_status.get("utilization", 0),
            "memory_usage_mb": random.uniform(100, 500),
            "cpu_usage_percent": random.uniform(20, 80),
            "error_count": 0,
            "success_rate": 1.0
        }
        
        return {"performance_stats": performance_stats}
    
    # æ„å»ºæµå¤„ç†å·¥ä½œæµ
    def build_stream_processing_workflow():
        workflow = StateGraph(StreamProcessingState)
        
        workflow.add_node("manage_buffer", manage_buffer)
        workflow.add_node("aggregate", real_time_aggregation)
        workflow.add_node("apply_rules", apply_processing_rules)
        workflow.add_node("detect_anomalies", detect_anomalies)
        workflow.add_node("performance_stats", calculate_performance_stats)
        
        workflow.set_entry_point("manage_buffer")
        workflow.add_edge("manage_buffer", "aggregate")
        
        # å¹¶è¡Œæ‰§è¡Œè§„åˆ™åº”ç”¨å’Œå¼‚å¸¸æ£€æµ‹
        workflow.add_edge("aggregate", "apply_rules")
        workflow.add_edge("aggregate", "detect_anomalies")
        
        workflow.add_edge("apply_rules", "performance_stats")
        workflow.add_edge("detect_anomalies", "performance_stats")
        workflow.add_edge("performance_stats", END)
        
        return workflow.compile()
    
    # æµ‹è¯•å‡½æ•°
    def test_stream_processing():
        print_step("æµ‹è¯•å®æ—¶æ•°æ®æµå¤„ç†")
        
        app = build_stream_processing_workflow()
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®æµ
        current_time = time.time()
        data_events = []
        
        # ç”Ÿæˆå„ç§ç±»å‹çš„äº‹ä»¶
        for i in range(100):
            event = {
                "event_id": f"event_{i}",
                "event_type": random.choice(["metric", "event", "log"]),
                "timestamp": current_time - random.uniform(0, 300),
                "data": {
                    "name": f"metric_{i % 10}",
                    "value": random.uniform(10, 1000),
                    "source": f"source_{i % 5}"
                },
                "priority": random.choice(["high", "medium", "low"])
            }
            data_events.append(event)
        
        # å®šä¹‰å¤„ç†è§„åˆ™
        processing_rules = [
            {
                "name": "high_metric_value",
                "conditions": [
                    {"field": "event_type", "operator": "==", "value": "metric"},
                    {"field": "data.value", "operator": ">", "value": 800}
                ],
                "severity": "high",
                "message": "Metric value is unusually high"
            },
            {
                "name": "error_log_detection",
                "conditions": [
                    {"field": "event_type", "operator": "==", "value": "log"},
                    {"field": "data.level", "operator": "==", "value": "error"}
                ],
                "severity": "medium",
                "message": "Error log detected"
            }
        ]
        
        initial_state = {
            "stream_id": f"stream_{int(time.time())}",
            "data_events": data_events,
            "processing_rules": processing_rules,
            "aggregated_results": {},
            "alerts": [],
            "performance_stats": {},
            "buffer_status": {},
            "error_log": []
        }
        
        result = app.invoke(initial_state)
        
        # æ˜¾ç¤ºç»“æœ
        aggregated_results = result.get("aggregated_results", {})
        alerts = result.get("alerts", [])
        performance_stats = result.get("performance_stats", {})
        buffer_status = result.get("buffer_status", {})
        
        print(f"\nğŸ“Š èšåˆç»“æœ:")
        print(f"  æ€»äº‹ä»¶æ•°: {aggregated_results.get('total_events', 0)}")
        print(f"  äº‹ä»¶ç±»å‹åˆ†å¸ƒ: {aggregated_results.get('event_types', {})}")
        print(f"  æ—¶é—´çª—å£: {aggregated_results.get('time_window', {})}")
        
        key_metrics = aggregated_results.get("key_metrics", {})
        if key_metrics:
            print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
            for metric, stats in list(key_metrics.items())[:3]:
                print(f"  {metric}: avg={stats.get('avg', 0):.2f}, min={stats.get('min', 0)}, max={stats.get('max', 0)}")
        
        print(f"\nğŸš¨ å‘Šè­¦ä¿¡æ¯:")
        print(f"  ç”Ÿæˆå‘Šè­¦æ•°: {len(alerts)}")
        for alert in alerts[:3]:
            print(f"  - {alert.get('severity', 'unknown')}: {alert.get('message', '')}")
        
        print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¤„ç†æ—¶é—´: {performance_stats.get('processing_time_seconds', 0):.3f}s")
        print(f"  äº‹ä»¶/ç§’: {performance_stats.get('events_per_second', 0):.1f}")
        print(f"  ç¼“å†²åŒºåˆ©ç”¨ç‡: {buffer_status.get('utilization', 0):.1%}")
        print(f"  CPUä½¿ç”¨ç‡: {performance_stats.get('cpu_usage_percent', 0):.1f}%")
    
    return test_stream_processing


# ================================
 ç»ƒä¹  3: è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ
# ================================

def exercise_3_adaptive_learning():
    """
    ç»ƒä¹  3: è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ
    
    è¦æ±‚:
    1. å®ç°åŠ¨æ€å­¦ä¹ è·¯å¾„æ¨è
    2. åŸºäºå­¦ä¹ æ•ˆæœçš„éš¾åº¦è°ƒæ•´
    3. å¤šç»´åº¦å­¦ä¹ è¯„ä¼°
    4. ä¸ªæ€§åŒ–å†…å®¹æ¨è
    5. å­¦ä¹ è¿›åº¦è·Ÿè¸ªå’Œåˆ†æ
    
    æŒ‘æˆ˜ç‚¹:
    - å­¦ä¹ æ•ˆæœè¯„ä¼°ç®—æ³•
    - éš¾åº¦é€‚åº”æ€§è°ƒæ•´
    - å­¦ä¹ è·¯å¾„ä¼˜åŒ–
    - ä¸ªæ€§åŒ–å»ºæ¨¡
    """
    
    class AdaptiveLearningState(TypedDict):
        learner_id: str
        current_session: Dict[str, Any]
        learning_history: List[Dict[str, Any]]
        knowledge_model: Dict[str, Any]
        current_difficulty: float
        recommended_content: List[Dict[str, Any]]
        learning_path: List[Dict[str, Any]]
        performance_metrics: Dict[str, Any]
        adaptation_log: List[Dict[str, Any]]
    
    # å­¦ä¹ è€…ç”»åƒå»ºæ¨¡
    def build_learner_profile(state: AdaptiveLearningState) -> AdaptiveLearningState:
        """æ„å»ºå­¦ä¹ è€…ç”»åƒ"""
        learner_id = state.get("learner_id", "")
        learning_history = state.get("learning_history", [])
        
        knowledge_model = {
            "learner_id": learner_id,
            "knowledge_domains": {},
            "skill_levels": {},
            "learning_style": {},
            "strengths": [],
            "weaknesses": [],
            "preferred_difficulty": 0.5,
            "engagement_level": 0.0,
            "completion_rate": 0.0
        }
        
        if learning_history:
            # åˆ†æå­¦ä¹ å†å²
            total_sessions = len(learning_history)
            completed_sessions = sum(1 for session in learning_history if session.get("completed", False))
            
            # è®¡ç®—å®Œæˆç‡
            knowledge_model["completion_rate"] = completed_sessions / total_sessions
            
            # åˆ†æçŸ¥è¯†é¢†åŸŸ
            domain_scores = {}
            for session in learning_history:
                domain = session.get("domain", "general")
                score = session.get("performance_score", 0)
                if domain not in domain_scores:
                    domain_scores[domain] = []
                domain_scores[domain].append(score)
            
            # è®¡ç®—å„é¢†åŸŸå¹³å‡åˆ†æ•°
            for domain, scores in domain_scores.items():
                avg_score = sum(scores) / len(scores)
                knowledge_model["knowledge_domains"][domain] = avg_score
                
                # è¯†åˆ«å¼ºé¡¹å’Œå¼±é¡¹
                if avg_score > 0.8:
                    knowledge_model["strengths"].append(domain)
                elif avg_score < 0.5:
                    knowledge_model["weaknesses"].append(domain)
            
            # åˆ†æå‚ä¸åº¦
            engagement_scores = [s.get("engagement_score", 0) for s in learning_history]
            knowledge_model["engagement_level"] = sum(engagement_scores) / len(engagement_scores)
            
            # æ¨æ–­å­¦ä¹ é£æ ¼
            knowledge_model["learning_style"] = infer_learning_style(learning_history)
        
        return {"knowledge_model": knowledge_model}
    
    def infer_learning_style(history: List[Dict[str, Any]]) -> Dict[str, float]:
        """æ¨æ–­å­¦ä¹ é£æ ¼"""
        styles = {
            "visual": 0.0,
            "auditory": 0.0,
            "kinesthetic": 0.0,
            "reading": 0.0
        }
        
        for session in history:
            session_type = session.get("session_type", "")
            performance = session.get("performance_score", 0)
            
            if "video" in session_type:
                styles["visual"] += performance
            elif "audio" in session_type:
                styles["auditory"] += performance
            elif "interactive" in session_type:
                styles["kinesthetic"] += performance
            elif "text" in session_type:
                styles["reading"] += performance
        
        # å½’ä¸€åŒ–
        total = sum(styles.values())
        if total > 0:
            for style in styles:
                styles[style] /= total
        
        return styles
    
    # éš¾åº¦è‡ªé€‚åº”
    def adaptive_difficulty_adjustment(state: AdaptiveLearningState) -> AdaptiveLearningState:
        """è‡ªé€‚åº”éš¾åº¦è°ƒæ•´"""
        knowledge_model = state.get("knowledge_model", {})
        current_session = state.get("current_session", {})
        learning_history = state.get("learning_history", [])
        
        # å½“å‰éš¾åº¦
        current_difficulty = state.get("current_difficulty", 0.5)
        
        # è·å–æœ€è¿‘çš„è¡¨ç°
        recent_sessions = learning_history[-5:]  # æœ€è¿‘5æ¬¡
        if len(recent_sessions) >= 3:
            recent_scores = [s.get("performance_score", 0) for s in recent_sessions]
            avg_recent_score = sum(recent_scores) / len(recent_scores)
            
            # æ ¹æ®è¡¨ç°è°ƒæ•´éš¾åº¦
            if avg_recent_score > 0.85:  # è¡¨ç°å¾ˆå¥½ï¼Œå¢åŠ éš¾åº¦
                new_difficulty = min(current_difficulty + 0.1, 1.0)
                reason = "high_performance"
            elif avg_recent_score < 0.5:  # è¡¨ç°è¾ƒå·®ï¼Œé™ä½éš¾åº¦
                new_difficulty = max(current_difficulty - 0.1, 0.1)
                reason = "low_performance"
            else:  # è¡¨ç°é€‚ä¸­ï¼Œä¿æŒéš¾åº¦
                new_difficulty = current_difficulty
                reason = "stable_performance"
            
            # è€ƒè™‘å­¦ä¹ è€…çš„åå¥½
            preferred_difficulty = knowledge_model.get("preferred_difficulty", 0.5)
            new_difficulty = 0.7 * new_difficulty + 0.3 * preferred_difficulty
            
            adaptation_log = state.get("adaptation_log", [])
            adaptation_log.append({
                "timestamp": datetime.now().isoformat(),
                "old_difficulty": current_difficulty,
                "new_difficulty": new_difficulty,
                "reason": reason,
                "recent_performance": avg_recent_score
            })
            
            return {
                "current_difficulty": new_difficulty,
                "adaptation_log": adaptation_log
            }
        
        return {}
    
    # å­¦ä¹ å†…å®¹æ¨è
    def recommend_learning_content(state: AdaptiveLearningState) -> AdaptiveLearningState:
        """æ¨èå­¦ä¹ å†…å®¹"""
        knowledge_model = state.get("knowledge_model", {})
        current_difficulty = state.get("current_difficulty", 0.5)
        
        # ç”Ÿæˆæ¨èå†…å®¹
        recommended_content = []
        
        # åŸºäºå¼±ç‚¹æ¨è
        weaknesses = knowledge_model.get("weaknesses", [])
        for domain in weaknesses:
            content = {
                "content_id": f"content_{domain}_{int(time.time())}",
                "domain": domain,
                "type": "tutorial",
                "difficulty": current_difficulty * 0.8,  # ä»ç¨ä½éš¾åº¦å¼€å§‹
                "estimated_time": random.randint(15, 45),
                "learning_objectives": [f"improve_{domain}"],
                "priority": "high"
            }
            recommended_content.append(content)
        
        # åŸºäºå¼ºé¡¹æ¨èè¿›é˜¶å†…å®¹
        strengths = knowledge_model.get("strengths", [])
        for domain in strengths:
            content = {
                "content_id": f"advanced_{domain}_{int(time.time())}",
                "domain": domain,
                "type": "advanced_exercise",
                "difficulty": min(current_difficulty * 1.2, 1.0),
                "estimated_time": random.randint(20, 60),
                "learning_objectives": [f"advance_{domain}"],
                "priority": "medium"
            }
            recommended_content.append(content)
        
        # åŸºäºå­¦ä¹ é£æ ¼æ¨è
        learning_style = knowledge_model.get("learning_style", {})
        preferred_style = max(learning_style.items(), key=lambda x: x[1])[0] if learning_style else "visual"
        
        style_based_content = {
            "content_id": f"style_based_{preferred_style}_{int(time.time())}",
            "domain": "general",
            "type": f"{preferred_style}_content",
            "difficulty": current_difficulty,
            "estimated_time": random.randint(10, 30),
            "learning_objectives": ["engagement_improvement"],
            "priority": "low"
        }
        recommended_content.append(style_based_content)
        
        return {"recommended_content": recommended_content}
    
    # å­¦ä¹ è·¯å¾„è§„åˆ’
    def generate_learning_path(state: AdaptiveLearningState) -> AdaptiveLearningState:
        """ç”Ÿæˆå­¦ä¹ è·¯å¾„"""
        recommended_content = state.get("recommended_content", [])
        knowledge_model = state.get("knowledge_model", {})
        
        # æŒ‰ä¼˜å…ˆçº§å’Œéš¾åº¦æ’åºå†…å®¹
        priority_order = {"high": 3, "medium": 2, "low": 1}
        sorted_content = sorted(
            recommended_content,
            key=lambda x: (priority_order.get(x["priority"], 0), x["difficulty"])
        )
        
        # ç”Ÿæˆå­¦ä¹ è·¯å¾„
        learning_path = []
        current_time = time.time()
        
        for i, content in enumerate(sorted_content):
            step = {
                "step_number": i + 1,
                "content": content,
                "estimated_duration": content.get("estimated_time", 30),
                "prerequisites": [],
                "learning_outcomes": content.get("learning_objectives", []),
                "scheduled_start": current_time + sum(step.get("estimated_duration", 0) for step in learning_path)
            }
            learning_path.append(step)
        
        return {"learning_path": learning_path}
    
    # æ€§èƒ½æŒ‡æ ‡è®¡ç®—
    def calculate_learning_metrics(state: AdaptiveLearningState) -> AdaptiveLearningState:
        """è®¡ç®—å­¦ä¹ æ€§èƒ½æŒ‡æ ‡"""
        learning_history = state.get("learning_history", [])
        knowledge_model = state.get("knowledge_model", {})
        learning_path = state.get("learning_path", [])
        
        performance_metrics = {
            "total_learning_time": sum(s.get("duration", 0) for s in learning_history),
            "average_session_score": 0,
            "improvement_rate": 0,
            "knowledge_growth": {},
            "engagement_trend": [],
            "goal_completion_rate": 0
        }
        
        if learning_history:
            # å¹³å‡åˆ†æ•°
            scores = [s.get("performance_score", 0) for s in learning_history]
            performance_metrics["average_session_score"] = sum(scores) / len(scores)
            
            # æ”¹è¿›ç‡
            if len(scores) >= 2:
                early_average = sum(scores[:len(scores)//2]) / (len(scores)//2)
                recent_average = sum(scores[len(scores)//2:]) / (len(scores) - len(scores)//2)
                performance_metrics["improvement_rate"] = (recent_average - early_average) / early_average if early_average > 0 else 0
            
            # çŸ¥è¯†æˆé•¿
            domain_scores = knowledge_model.get("knowledge_domains", {})
            for domain, score in domain_scores.items():
                performance_metrics["knowledge_growth"][domain] = {
                    "current_level": score,
                    "target_level": min(score + 0.2, 1.0),
                    "improvement_needed": max(0, 1.0 - score)
                }
            
            # å‚ä¸åº¦è¶‹åŠ¿
            engagement_scores = [s.get("engagement_score", 0) for s in learning_history[-10:]]
            performance_metrics["engagement_trend"] = engagement_scores
        
        # ç›®æ ‡å®Œæˆç‡
        if learning_path:
            total_steps = len(learning_path)
            completed_steps = sum(1 for s in learning_history if s.get("completed", False))
            performance_metrics["goal_completion_rate"] = completed_steps / total_steps if total_steps > 0 else 0
        
        return {"performance_metrics": performance_metrics}
    
    # æ„å»ºè‡ªé€‚åº”å­¦ä¹ å·¥ä½œæµ
    def build_adaptive_learning_workflow():
        workflow = StateGraph(AdaptiveLearningState)
        
        workflow.add_node("build_profile", build_learner_profile)
        workflow.add_node("adjust_difficulty", adaptive_difficulty_adjustment)
        workflow.add_node("recommend_content", recommend_learning_content)
        workflow.add_node("generate_path", generate_learning_path)
        workflow.add_node("calculate_metrics", calculate_learning_metrics)
        
        workflow.set_entry_point("build_profile")
        workflow.add_edge("build_profile", "adjust_difficulty")
        workflow.add_edge("adjust_difficulty", "recommend_content")
        workflow.add_edge("recommend_content", "generate_path")
        workflow.add_edge("generate_path", "calculate_metrics")
        workflow.add_edge("calculate_metrics", END)
        
        return workflow.compile()
    
    # æµ‹è¯•å‡½æ•°
    def test_adaptive_learning():
        print_step("æµ‹è¯•è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ")
        
        app = build_adaptive_learning_workflow()
        
        # ç”Ÿæˆæ¨¡æ‹Ÿå­¦ä¹ å†å²
        learning_history = []
        for i in range(20):
            session = {
                "session_id": f"session_{i}",
                "domain": random.choice(["mathematics", "programming", "science", "language"]),
                "session_type": random.choice(["video", "interactive", "text", "audio"]),
                "duration": random.randint(10, 60),
                "performance_score": random.uniform(0.3, 0.95),
                "engagement_score": random.uniform(0.4, 0.9),
                "completed": random.random() > 0.1,
                "timestamp": time.time() - random.uniform(0, 30*24*3600)
            }
            learning_history.append(session)
        
        current_session = {
            "session_id": f"current_session_{int(time.time())}",
            "start_time": time.time(),
            "domain": "programming"
        }
        
        initial_state = {
            "learner_id": "learner_123",
            "current_session": current_session,
            "learning_history": learning_history,
            "knowledge_model": {},
            "current_difficulty": 0.5,
            "recommended_content": [],
            "learning_path": [],
            "performance_metrics": {},
            "adaptation_log": []
        }
        
        result = app.invoke(initial_state)
        
        # æ˜¾ç¤ºç»“æœ
        knowledge_model = result.get("knowledge_model", {})
        current_difficulty = result.get("current_difficulty", 0.5)
        recommended_content = result.get("recommended_content", [])
        learning_path = result.get("learning_path", [])
        performance_metrics = result.get("performance_metrics", {})
        adaptation_log = result.get("adaptation_log", [])
        
        print(f"\nğŸ‘¤ å­¦ä¹ è€…ç”»åƒ:")
        print(f"  å®Œæˆç‡: {knowledge_model.get('completion_rate', 0):.1%}")
        print(f"  å‚ä¸åº¦: {knowledge_model.get('engagement_level', 0):.1%}")
        print(f"  å¼ºé¡¹: {knowledge_model.get('strengths', [])}")
        print(f"  å¼±é¡¹: {knowledge_model.get('weaknesses', [])}")
        
        learning_style = knowledge_model.get('learning_style', {})
        if learning_style:
            preferred_style = max(learning_style.items(), key=lambda x: x[1])[0]
            print(f"  å­¦ä¹ é£æ ¼: {preferred_style}")
        
        print(f"\nğŸ¯ å½“å‰éš¾åº¦: {current_difficulty:.2f}")
        
        if adaptation_log:
            latest_adaptation = adaptation_log[-1]
            print(f"  éš¾åº¦è°ƒæ•´: {latest_adaptation.get('reason', '')} ({latest_adaptation.get('old_difficulty', 0):.2f} â†’ {latest_adaptation.get('new_difficulty', 0):.2f})")
        
        print(f"\nğŸ“š æ¨èå†…å®¹:")
        for content in recommended_content[:5]:
            priority = content.get("priority", "medium")
            domain = content.get("domain", "general")
            content_type = content.get("type", "tutorial")
            print(f"  - {domain} ({content_type}) - {priority} priority")
        
        print(f"\nğŸ—ºï¸ å­¦ä¹ è·¯å¾„:")
        print(f"  æ€»æ­¥éª¤: {len(learning_path)}")
        estimated_total_time = sum(step.get("estimated_duration", 0) for step in learning_path)
        print(f"  é¢„è®¡æ€»æ—¶é•¿: {estimated_total_time} åˆ†é’Ÿ")
        
        print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å¹³å‡åˆ†æ•°: {performance_metrics.get('average_session_score', 0):.2f}")
        print(f"  æ”¹è¿›ç‡: {performance_metrics.get('improvement_rate', 0):.1%}")
        print(f"  ç›®æ ‡å®Œæˆç‡: {performance_metrics.get('goal_completion_rate', 0):.1%}")
        
        knowledge_growth = performance_metrics.get("knowledge_growth", {})
        if knowledge_growth:
            print(f"  çŸ¥è¯†æˆé•¿:")
            for domain, growth in list(knowledge_growth.items())[:3]:
                current = growth.get("current_level", 0)
                print(f"    {domain}: {current:.2f}")
    
    return test_adaptive_learning


# ================================
 ä¸»æµ‹è¯•å‡½æ•°
# ================================

def run_advanced_exercises():
    """è¿è¡Œæ‰€æœ‰é«˜çº§ç»ƒä¹ """
    print("ğŸ¯ LangGraph é«˜çº§é—®é¢˜è§£å†³ç»ƒä¹ ")
    print("=" * 60)
    
    exercises = [
        ("æ™ºèƒ½æ¨èç³»ç»Ÿ", exercise_1_recommendation_system),
        ("å®æ—¶æ•°æ®æµå¤„ç†", exercise_2_stream_processing),
        ("è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ", exercise_3_adaptive_learning)
    ]
    
    while True:
        print("\nè¯·é€‰æ‹©é«˜çº§ç»ƒä¹ :")
        for i, (name, func) in enumerate(exercises, 1):
            print(f"{i}. {name}")
        print("4. è¿è¡Œæ‰€æœ‰ç»ƒä¹ ")
        print("0. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-4): ").strip()
        
        if choice == "1":
            exercises[0][1]()
        elif choice == "2":
            exercises[1][1]()
        elif choice == "3":
            exercises[2][1]()
        elif choice == "4":
            print("\n" + "="*50)
            print("è¿è¡Œæ‰€æœ‰é«˜çº§ç»ƒä¹ ")
            print("="*50)
            for name, func in exercises:
                print(f"\n{'='*20} {name} {'='*20}")
                func()
                time.sleep(2)
        elif choice == "0":
            print_step("æ„Ÿè°¢å®Œæˆé«˜çº§ç»ƒä¹ ï¼")
            break
        else:
            print_error("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
    
    print_result("é«˜çº§é—®é¢˜è§£å†³ç»ƒä¹ å®Œæˆï¼")


if __name__ == "__main__":
    run_advanced_exercises()
    
    print_step("""
é«˜çº§ç»ƒä¹ å®Œæˆæ€»ç»“:

1. æ™ºèƒ½æ¨èç³»ç»Ÿ
   - å®ç°äº†ç”¨æˆ·ç”»åƒå»ºæ¨¡
   - æ”¯æŒå¤šç§æ¨èç®—æ³•
   - åŒ…å«A/Bæµ‹è¯•åŠŸèƒ½
   - æ€§èƒ½è¯„ä¼°å’Œä¼˜åŒ–

2. å®æ—¶æ•°æ®æµå¤„ç†
   - é«˜å¹¶å‘æ•°æ®å¤„ç†
   - èƒŒå‹æœºåˆ¶
   - å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦
   - æ€§èƒ½ç›‘æ§

3. è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ
   - å­¦ä¹ è€…ç”»åƒåˆ†æ
   - éš¾åº¦è‡ªé€‚åº”è°ƒæ•´
   - ä¸ªæ€§åŒ–å†…å®¹æ¨è
   - å­¦ä¹ è·¯å¾„ä¼˜åŒ–

è¿™äº›ç»ƒä¹ å±•ç¤ºäº†LangGraphåœ¨å¤„ç†å¤æ‚ä¸šåŠ¡é€»è¾‘ã€
é«˜æ€§èƒ½è¦æ±‚å’Œæ™ºèƒ½åŒ–åº”ç”¨æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚
    """)